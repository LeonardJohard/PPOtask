{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZsIVxC1fhJx"
      },
      "source": [
        "Report\n",
        "0. Most time unfortunately spent fixing a broken Anaconda installation from years ago...\n",
        "1. Failed to install various version of MuJoCo on Windows.\n",
        "2. Installed MuJoCo on colab, minus the viewer.\n",
        "3. Found a broken ready-made PPO implementation (99 % of code). Modified it.\n",
        "4. Wrote a reward function and light hyperparameter tuning. The reward idea is a product of upright angle and a Gaussian weighting the distance to the target point.\n",
        "5. The code is admittedly resarch code, slow and visualization is broken, but reward seems high enough that I guess it kind of works. I deserve minus points in code quality, though.\n",
        "What would speed up training:\n",
        "A) Hyperparameter tuning\n",
        "B) Use differentiable sim/surrogate if allowed\n",
        "C) Use bootstrapping for critic rather than MC\n",
        "D) Reimplement compiled training loop\n",
        "E) Use deterministic policy gradients\n",
        "F) Code optimizations such as minimizing allocations, use eligibility traces etc\n",
        "G) Generative world model\n",
        "H) A few secret tricks up my sleeve\n",
        "6. Rewriting the entire training loop in Julia for testing some new algorithm, but this needs a day or two and has to wait for the holiday.The new solver packages from JuliaSim/MIT with surrogate modeling support could accelerate training and that the upcoming Julia AoT compiler developments could also greatly facilitate deployment directly to various controllers. Could be fun to investigate in any case.\n",
        "7. Will modify code of the MuJoCo model to make it POMDP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4VJcUT2GlJz"
      },
      "source": [
        "################################################################################\n",
        "> # **Install Dependencies**\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rbpSQTflGlAr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e0cf4fe-9733-4d3e-dcac-a747306a7af7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swig\n",
            "  Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.3.0\n",
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "############ install compatible version of OpenAI roboschool and gym ###########\n",
        "\n",
        "!pip install swig\n",
        "#!pip install mujoco\n",
        "#Did not work\n",
        "\n",
        "!pip install gymnasium\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzZairIiGQ11"
      },
      "source": [
        "################################################################################\n",
        "> # **Introduction**\n",
        "> The notebook is divided into 5 major parts :\n",
        "\n",
        "*   **Part I** : define actor-critic network and PPO algorithm\n",
        "*   **Part II** : train PPO algorithm and save network weights and log files\n",
        "*   **Part III** : load (preTrained) network weights and test PPO algorithm\n",
        "*   **Part IV** : load log files and plot graphs\n",
        "*   **Part V** : install xvbf, load (preTrained) network weights and save images for gif and then generate gif\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s37cJXAYGrTY"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - I**\n",
        "\n",
        "*   define actor critic networks\n",
        "*   define PPO algorithm\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UT6VUBg-F8Zm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d19beb1-a6a8-4ece-b1fd-d59d5b3db4a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Selecting previously unselected package libglx-dev:amd64.\n",
            "(Reading database ... 123632 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libglx-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglx-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgl-dev:amd64.\n",
            "Preparing to unpack .../01-libgl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgl-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libegl-dev:amd64.\n",
            "Preparing to unpack .../02-libegl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libegl-dev:amd64 (1.4.0-1) ...\n",
            "Preparing to unpack .../03-libegl-mesa0_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\n",
            "Unpacking libegl-mesa0:amd64 (23.2.1-1ubuntu3.1~22.04.3) over (23.2.1-1ubuntu3.1~22.04.2) ...\n",
            "Preparing to unpack .../04-libgbm1_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\n",
            "Unpacking libgbm1:amd64 (23.2.1-1ubuntu3.1~22.04.3) over (23.2.1-1ubuntu3.1~22.04.2) ...\n",
            "Preparing to unpack .../05-libgl1-mesa-dri_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\n",
            "Unpacking libgl1-mesa-dri:amd64 (23.2.1-1ubuntu3.1~22.04.3) over (23.2.1-1ubuntu3.1~22.04.2) ...\n",
            "Preparing to unpack .../06-libglx-mesa0_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\n",
            "Unpacking libglx-mesa0:amd64 (23.2.1-1ubuntu3.1~22.04.3) over (23.2.1-1ubuntu3.1~22.04.2) ...\n",
            "Preparing to unpack .../07-libglapi-mesa_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\n",
            "Unpacking libglapi-mesa:amd64 (23.2.1-1ubuntu3.1~22.04.3) over (23.2.1-1ubuntu3.1~22.04.2) ...\n",
            "Selecting previously unselected package libgl1-mesa-glx:amd64.\n",
            "Preparing to unpack .../08-libgl1-mesa-glx_23.0.4-0ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking libgl1-mesa-glx:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n",
            "Selecting previously unselected package libgles1:amd64.\n",
            "Preparing to unpack .../09-libgles1_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgles1:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgles-dev:amd64.\n",
            "Preparing to unpack .../10-libgles-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgles-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libglvnd-core-dev:amd64.\n",
            "Preparing to unpack .../11-libglvnd-core-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglvnd-core-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libopengl-dev:amd64.\n",
            "Preparing to unpack .../12-libopengl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libopengl-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libglvnd-dev:amd64.\n",
            "Preparing to unpack .../13-libglvnd-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglvnd-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgl1-mesa-dev:amd64.\n",
            "Preparing to unpack .../14-libgl1-mesa-dev_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\n",
            "Unpacking libgl1-mesa-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Selecting previously unselected package libglu1-mesa:amd64.\n",
            "Preparing to unpack .../15-libglu1-mesa_9.0.2-1_amd64.deb ...\n",
            "Unpacking libglu1-mesa:amd64 (9.0.2-1) ...\n",
            "Selecting previously unselected package libglu1-mesa-dev:amd64.\n",
            "Preparing to unpack .../16-libglu1-mesa-dev_9.0.2-1_amd64.deb ...\n",
            "Unpacking libglu1-mesa-dev:amd64 (9.0.2-1) ...\n",
            "Selecting previously unselected package libglew-dev:amd64.\n",
            "Preparing to unpack .../17-libglew-dev_2.2.0-4_amd64.deb ...\n",
            "Unpacking libglew-dev:amd64 (2.2.0-4) ...\n",
            "Selecting previously unselected package libglfw3:amd64.\n",
            "Preparing to unpack .../18-libglfw3_3.3.6-1_amd64.deb ...\n",
            "Unpacking libglfw3:amd64 (3.3.6-1) ...\n",
            "Selecting previously unselected package patchelf.\n",
            "Preparing to unpack .../19-patchelf_0.14.3-1_amd64.deb ...\n",
            "Unpacking patchelf (0.14.3-1) ...\n",
            "Selecting previously unselected package libosmesa6:amd64.\n",
            "Preparing to unpack .../20-libosmesa6_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\n",
            "Unpacking libosmesa6:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Selecting previously unselected package libosmesa6-dev:amd64.\n",
            "Preparing to unpack .../21-libosmesa6-dev_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\n",
            "Unpacking libosmesa6-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up libglfw3:amd64 (3.3.6-1) ...\n",
            "Setting up libglvnd-core-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libgbm1:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up libgles1:amd64 (1.4.0-1) ...\n",
            "Setting up libglapi-mesa:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up libglx-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libglu1-mesa:amd64 (9.0.2-1) ...\n",
            "Setting up libopengl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up patchelf (0.14.3-1) ...\n",
            "Setting up libgl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libosmesa6:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up libegl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libgl1-mesa-dri:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up libegl-mesa0:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up libglu1-mesa-dev:amd64 (9.0.2-1) ...\n",
            "Setting up libosmesa6-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up libgles-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libglx-mesa0:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up libglvnd-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libglew-dev:amd64 (2.2.0-4) ...\n",
            "Setting up libgl1-mesa-glx:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n",
            "Setting up libgl1-mesa-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "Collecting mujoco\n",
            "  Downloading mujoco-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mujoco) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.10/dist-packages (from mujoco) (1.11.0)\n",
            "Collecting glfw (from mujoco)\n",
            "  Downloading glfw-2.8.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mujoco) (1.26.4)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.10/dist-packages (from mujoco) (3.1.7)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco) (2024.10.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco) (6.4.5)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco) (4.12.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco) (3.21.0)\n",
            "Downloading mujoco-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading glfw-2.8.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: glfw, mujoco\n",
            "Successfully installed glfw-2.8.0 mujoco-3.2.6\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "############################### Import libraries ###############################\n",
        "\n",
        "import os\n",
        "#Mujoco that works in colab\n",
        "if not os.path.exists('.mujoco_setup_complete'):\n",
        "  # Get the prereqs\n",
        "  !apt-get -qq update\n",
        "  !apt-get -qq install -y libosmesa6-dev libgl1-mesa-glx libglfw3 libgl1-mesa-dev libglew-dev patchelf\n",
        "  # Get Mujoco\n",
        "  !mkdir ~/.mujoco\n",
        "  !wget -q https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz -O mujoco.tar.gz\n",
        "  !tar -zxf mujoco.tar.gz -C \"$HOME/.mujoco\"\n",
        "  !rm mujoco.tar.gz\n",
        "  # Add it to the actively loaded path and the bashrc path (these only do so much)\n",
        "  !echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HOME/.mujoco/mujoco210/bin' >> ~/.bashrc\n",
        "  !echo 'export LD_PRELOAD=$LD_PRELOAD:/usr/lib/x86_64-linux-gnu/libGLEW.so' >> ~/.bashrc\n",
        "  # THE ANNOYING ONE, FORCE IT INTO LDCONFIG SO WE ACTUALLY GET ACCESS TO IT THIS SESSION\n",
        "  !echo \"/root/.mujoco/mujoco210/bin\" > /etc/ld.so.conf.d/mujoco_ld_lib_path.conf\n",
        "  !ldconfig\n",
        "  # Install Mujoco-py\n",
        "  !pip3 install -U 'mujoco'\n",
        "  # run once\n",
        "  !touch .mujoco_setup_complete\n",
        "\n",
        "try:\n",
        "  if _mujoco_run_once:\n",
        "    pass\n",
        "except NameError:\n",
        "  _mujoco_run_once = False\n",
        "if not _mujoco_run_once:\n",
        "  # Add it to the actively loaded path and the bashrc path (these only do so much)\n",
        "  try:\n",
        "    os.environ['LD_LIBRARY_PATH']=os.environ['LD_LIBRARY_PATH'] + ':/root/.mujoco/mujoco210/bin'\n",
        "  except KeyError:\n",
        "    os.environ['LD_LIBRARY_PATH']='/root/.mujoco/mujoco210/bin'\n",
        "  try:\n",
        "    os.environ['LD_PRELOAD']=os.environ['LD_PRELOAD'] + ':/usr/lib/x86_64-linux-gnu/libGLEW.so'\n",
        "  except KeyError:\n",
        "    os.environ['LD_PRELOAD']='/usr/lib/x86_64-linux-gnu/libGLEW.so'\n",
        "  # presetup so we don't see output on first env initialization\n",
        "  import mujoco\n",
        "  import mujoco.viewer\n",
        "  _mujoco_run_once = True\n",
        "import glob\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import MultivariateNormal\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import gymnasium as gym\n",
        "import gymnasium.spaces as spaces\n",
        "import math\n",
        "\n",
        "import time\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0al4F7pEfhJ1"
      },
      "outputs": [],
      "source": [
        "class InvertedPendulumEnv:\n",
        "    xml_env = \"\"\"\n",
        "    <mujoco model=\"inverted pendulum\">\n",
        "            <visual>\n",
        "            <headlight diffuse=\"0.6 0.6 0.6\" ambient=\"0.3 0.3 0.3\" specular=\"0 0 0\"/>\n",
        "            <rgba haze=\"0.15 0.25 0.35 1\"/>\n",
        "            <global azimuth=\"160\" elevation=\"-20\"/>\n",
        "        </visual>\n",
        "\n",
        "        <asset>\n",
        "            <texture type=\"skybox\" builtin=\"gradient\" rgb1=\"0.3 0.5 0.7\" rgb2=\"0 0 0\" width=\"512\" height=\"3072\"/>\n",
        "        </asset>\n",
        "        <compiler inertiafromgeom=\"true\"/>\n",
        "        <default>\n",
        "            <joint armature=\"0\" damping=\"1\" limited=\"true\"/>\n",
        "            <geom contype=\"0\" friction=\"1 0.1 0.1\" rgba=\"0.0 0.7 0 1\"/>\n",
        "            <tendon/>\n",
        "            <motor ctrlrange=\"-3 3\"/>\n",
        "        </default>\n",
        "        <option gravity=\"0 0 -9.81\" integrator=\"RK4\" timestep=\"0.02\"/>\n",
        "        <size nstack=\"3000\"/>\n",
        "        <worldbody>\n",
        "            <light pos=\"0 0 3.5\" dir=\"0 0 -1\" directional=\"true\"/>\n",
        "            <!--geom name=\"ground\" type=\"plane\" pos=\"0 0 0\" /-->\n",
        "            <geom name=\"rail\" pos=\"0 0 0\" quat=\"0.707 0 0.707 0\" rgba=\"0.3 0.3 0.7 1\" size=\"0.02 1\" type=\"capsule\" group=\"3\"/>\n",
        "            <body name=\"cart\" pos=\"0 0 0\">\n",
        "                <joint axis=\"1 0 0\" limited=\"true\" name=\"slider\" pos=\"0 0 0\" range=\"-1 1\" type=\"slide\"/>\n",
        "                <geom name=\"cart\" pos=\"0 0 0\" quat=\"0.707 0 0.707 0\" size=\"0.1 0.1\" type=\"capsule\"/>\n",
        "                <body name=\"pole\" pos=\"0 0 0\">\n",
        "                    <joint axis=\"0 1 0\" name=\"hinge\" pos=\"0 0 0\" range=\"-100000 100000\" type=\"hinge\"/>\n",
        "                    <geom fromto=\"0 0 0 0.001 0 0.6\" name=\"cpole\" rgba=\"0 0.7 0.7 1\" size=\"0.049 0.3\" type=\"capsule\"/>\n",
        "                </body>\n",
        "            </body>\n",
        "        </worldbody>\n",
        "        <actuator>\n",
        "            <motor ctrllimited=\"true\" ctrlrange=\"-3 3\" gear=\"100\" joint=\"slider\" name=\"slide\"/>\n",
        "        </actuator>\n",
        "    </mujoco>\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "    ):\n",
        "        self.init_qpos = np.zeros(2)\n",
        "        self.init_qvel = np.zeros(2)\n",
        "        self.model = mujoco.MjModel.from_xml_string(InvertedPendulumEnv.xml_env)\n",
        "        self.data = mujoco.MjData(self.model)\n",
        "        #self.viewer = mujoco.viewer.launch_passive(self.model, self.data) #Headless mode required in colab\n",
        "        self.reset_model()\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(4,), dtype=np.float64)\n",
        "        self.action_space = spaces.Box(low=-1, high=1, shape=(1,), dtype=np.float64)\n",
        "\n",
        "    def step(self, a):\n",
        "        self.data.ctrl = a\n",
        "        mujoco.mj_step(self.model, self.data)\n",
        "        #self.viewer.sync() #Headless mode required in colab\n",
        "        reward = 0.0025*math.cos(self.data.qpos[1]) * math.exp(-((self.data.qpos[0] - 0.2)/6)**2)\n",
        "        ob = self.obs()\n",
        "        terminated = bool(not np.isfinite(ob).all())\n",
        "        return ob, reward, terminated\n",
        "\n",
        "    def obs(self):\n",
        "        return np.concatenate([self.data.qpos, self.data.qvel]).ravel()\n",
        "\n",
        "    def reset_model(self):\n",
        "        self.data.qpos = self.init_qpos\n",
        "        self.data.qvel = self.init_qvel\n",
        "        self.data.qpos[1] = 3.14  # Set the pole to be facing down\n",
        "        return self.obs()\n",
        "\n",
        "    def set_dt(self, new_dt):\n",
        "        \"\"\"Sets simulations step\"\"\"\n",
        "        self.model.opt.timestep = new_dt\n",
        "\n",
        "    def draw_ball(self, position, color=[1, 0, 0, 1], radius=0.01):\n",
        "        mujoco.mjv_initGeom(\n",
        "            #self.viewer.user_scn.geoms[0], #Headless mode required in colab\n",
        "            type=mujoco.mjtGeom.mjGEOM_SPHERE,\n",
        "            size=[radius, 0, 0],\n",
        "            pos=np.array(position),\n",
        "            mat=np.eye(3).flatten(),\n",
        "            rgba=np.array(color),\n",
        "        )\n",
        "        #self.viewer.user_scn.ngeom = 1 #Headless mode required in colab\n",
        "\n",
        "    def reset(self):\n",
        "      return self.reset_model()\n",
        "\n",
        "    @property\n",
        "    def current_time(self):\n",
        "        return self.data.time\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "env = InvertedPendulumEnv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wVm0rZgifhJ1",
        "outputId": "d07fb4f3-d7ae-4f8c-eb68-ce7e497fb588",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "Device set to : cpu\n",
            "============================================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "################################## set device ##################################\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "# set device to cpu or cuda\n",
        "device = torch.device('cpu')\n",
        "\n",
        "if(torch.cuda.is_available()):\n",
        "    device = torch.device('cuda:0')\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n",
        "else:\n",
        "    print(\"Device set to : cpu\")\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "F94S_QmNfhJ1"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "################################## PPO Policy ##################################\n",
        "\n",
        "\n",
        "class RolloutBuffer:\n",
        "    def __init__(self):\n",
        "        self.actions = []\n",
        "        self.states = []\n",
        "        self.logprobs = []\n",
        "        self.rewards = []\n",
        "        self.state_values = []\n",
        "        self.is_terminals = []\n",
        "\n",
        "\n",
        "    def clear(self):\n",
        "        del self.actions[:]\n",
        "        del self.states[:]\n",
        "        del self.logprobs[:]\n",
        "        del self.rewards[:]\n",
        "        del self.state_values[:]\n",
        "        del self.is_terminals[:]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gBXnfkkGfhJ2"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, has_continuous_action_space, action_std_init):\n",
        "        super(ActorCritic, self).__init__()\n",
        "\n",
        "        self.has_continuous_action_space = has_continuous_action_space\n",
        "\n",
        "        if has_continuous_action_space:\n",
        "            self.action_dim = action_dim\n",
        "            self.action_var = torch.full((action_dim,), action_std_init * action_std_init).to(device)\n",
        "\n",
        "        # actor\n",
        "        if has_continuous_action_space :\n",
        "            self.actor = nn.Sequential(\n",
        "                            nn.Linear(state_dim, 64),\n",
        "                            nn.Mish(),\n",
        "                            nn.Linear(64, 64),\n",
        "                            nn.Mish(),\n",
        "                            nn.Linear(64, action_dim)\n",
        "                        )\n",
        "        else:\n",
        "            self.actor = nn.Sequential(\n",
        "                            nn.Linear(state_dim, 64),\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(64, 64),\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(64, action_dim),\n",
        "                            nn.Softmax(dim=-1)\n",
        "                        )\n",
        "\n",
        "\n",
        "        # critic\n",
        "        self.critic = nn.Sequential(\n",
        "                        nn.Linear(state_dim, 64),\n",
        "                        nn.Mish(),\n",
        "                        nn.Linear(64, 64),\n",
        "                        nn.Mish(),\n",
        "                        nn.Linear(64, 1)\n",
        "                    )\n",
        "\n",
        "    def set_action_std(self, new_action_std):\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            self.action_var = torch.full((self.action_dim,), new_action_std * new_action_std).to(device)\n",
        "        else:\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "            print(\"WARNING : Calling ActorCritic::set_action_std() on discrete action space policy\")\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "    def forward(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    def act(self, state):\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            action_mean = self.actor(state)\n",
        "            cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)\n",
        "            dist = MultivariateNormal(action_mean, cov_mat)\n",
        "        else:\n",
        "            action_probs = self.actor(state)\n",
        "            dist = Categorical(action_probs)\n",
        "\n",
        "        action = dist.sample()\n",
        "        action_logprob = dist.log_prob(action)\n",
        "        state_val = self.critic(state)\n",
        "\n",
        "        return action.detach(), action_logprob.detach(), state_val.detach()\n",
        "\n",
        "\n",
        "    def evaluate(self, state, action):\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            action_mean = self.actor(state)\n",
        "            action_var = self.action_var.expand_as(action_mean)\n",
        "            cov_mat = torch.diag_embed(action_var).to(device)\n",
        "            dist = MultivariateNormal(action_mean, cov_mat)\n",
        "\n",
        "            # for single action continuous environments\n",
        "            if self.action_dim == 1:\n",
        "                action = action.reshape(-1, self.action_dim)\n",
        "\n",
        "        else:\n",
        "            action_probs = self.actor(state)\n",
        "            dist = Categorical(action_probs)\n",
        "\n",
        "        action_logprobs = dist.log_prob(action)\n",
        "        dist_entropy = dist.entropy()\n",
        "        state_values = self.critic(state)\n",
        "\n",
        "        return action_logprobs, state_values, dist_entropy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gycJXbCMfhJ2"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class PPO:\n",
        "    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std_init=0.6):\n",
        "\n",
        "        self.has_continuous_action_space = has_continuous_action_space\n",
        "\n",
        "        if has_continuous_action_space:\n",
        "            self.action_std = action_std_init\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.eps_clip = eps_clip\n",
        "        self.K_epochs = K_epochs\n",
        "\n",
        "        self.buffer = RolloutBuffer()\n",
        "\n",
        "        self.policy = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
        "        self.optimizer = torch.optim.Adam([\n",
        "                        {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
        "                        {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
        "                    ])\n",
        "\n",
        "        self.policy_old = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        self.MseLoss = nn.MSELoss()\n",
        "\n",
        "\n",
        "    def set_action_std(self, new_action_std):\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            self.action_std = new_action_std\n",
        "            self.policy.set_action_std(new_action_std)\n",
        "            self.policy_old.set_action_std(new_action_std)\n",
        "\n",
        "        else:\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "            print(\"WARNING : Calling PPO::set_action_std() on discrete action space policy\")\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "    def decay_action_std(self, action_std_decay_rate, min_action_std):\n",
        "        print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            self.action_std = self.action_std - action_std_decay_rate\n",
        "            self.action_std = round(self.action_std, 4)\n",
        "            if (self.action_std <= min_action_std):\n",
        "                self.action_std = min_action_std\n",
        "                print(\"setting actor output action_std to min_action_std : \", self.action_std)\n",
        "            else:\n",
        "                print(\"setting actor output action_std to : \", self.action_std)\n",
        "            self.set_action_std(self.action_std)\n",
        "\n",
        "        else:\n",
        "            print(\"WARNING : Calling PPO::decay_action_std() on discrete action space policy\")\n",
        "\n",
        "        print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "    def select_action(self, state):\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            with torch.no_grad():\n",
        "                #state = torch.FloatTensor(state).to(device) #L Changed for below:\n",
        "                state = torch.tensor(state,dtype=torch.float32).to(device)\n",
        "                action, action_logprob, state_val = self.policy_old.act(state)\n",
        "\n",
        "            self.buffer.states.append(state)\n",
        "            self.buffer.actions.append(action)\n",
        "            self.buffer.logprobs.append(action_logprob)\n",
        "            self.buffer.state_values.append(state_val)\n",
        "\n",
        "            return action.detach().cpu().numpy().flatten()\n",
        "\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                #state = torch.FloatTensor(state).to(device) #L Changed for below:\n",
        "                state = torch.tensor(state).to(device)\n",
        "                action, action_logprob, state_val = self.policy_old.act(state)\n",
        "\n",
        "            self.buffer.states.append(state)\n",
        "            self.buffer.actions.append(action)\n",
        "            self.buffer.logprobs.append(action_logprob)\n",
        "            self.buffer.state_values.append(state_val)\n",
        "\n",
        "            return action.item()\n",
        "\n",
        "\n",
        "    def update(self):\n",
        "\n",
        "        # Monte Carlo estimate of returns\n",
        "        rewards = []\n",
        "        discounted_reward = 0\n",
        "        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n",
        "            if is_terminal:\n",
        "                discounted_reward = 0\n",
        "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
        "            rewards.insert(0, discounted_reward)\n",
        "\n",
        "        # Normalizing the rewards\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
        "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
        "\n",
        "        # convert list to tensor\n",
        "        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)\n",
        "        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)\n",
        "        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach().to(device)\n",
        "        old_state_values = torch.squeeze(torch.stack(self.buffer.state_values, dim=0)).detach().to(device)\n",
        "\n",
        "        # calculate advantages\n",
        "        advantages = rewards.detach() - old_state_values.detach()\n",
        "\n",
        "\n",
        "        # Optimize policy for K epochs\n",
        "        for _ in range(self.K_epochs):\n",
        "\n",
        "            # Evaluating old actions and values\n",
        "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
        "\n",
        "            # match state_values tensor dimensions with rewards tensor\n",
        "            state_values = torch.squeeze(state_values)\n",
        "\n",
        "            # Finding the ratio (pi_theta / pi_theta__old)\n",
        "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
        "\n",
        "            # Finding Surrogate Loss\n",
        "            surr1 = ratios * advantages\n",
        "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
        "\n",
        "            # final loss of clipped objective PPO\n",
        "            loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, rewards)# - 0.01 * dist_entropy\n",
        "\n",
        "            # take gradient step\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        # Copy new weights into old policy\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        # clear buffer\n",
        "        self.buffer.clear()\n",
        "\n",
        "\n",
        "    def save(self, checkpoint_path):\n",
        "        torch.save(self.policy_old.state_dict(), checkpoint_path)\n",
        "\n",
        "\n",
        "    def load(self, checkpoint_path):\n",
        "        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
        "        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-xCb_EyxF8cF"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "################################# End of Part I ################################\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr-ZjT_CGyEi"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - II**\n",
        "\n",
        "*   train PPO algorithm on environments\n",
        "*   save preTrained networks weights and log files\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YY1-DzVCF8eh",
        "outputId": "0d1cf9df-1509-416a-882a-e07b5437c9f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "training environment name : Inverted Pendulum in MuJoCo\n",
            "current logging run number for Inverted Pendulum in MuJoCo :  0\n",
            "logging at : PPO_logs/Inverted Pendulum in MuJoCo//PPO_Inverted Pendulum in MuJoCo_log_0.csv\n",
            "save checkpoint path : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "--------------------------------------------------------------------------------------------\n",
            "max training timesteps :  2000000\n",
            "max timesteps per episode :  400\n",
            "model saving frequency : 20000 timesteps\n",
            "log frequency : 800 timesteps\n",
            "printing average reward over episodes in last : 1600 timesteps\n",
            "--------------------------------------------------------------------------------------------\n",
            "state space dimension :  4\n",
            "action space dimension :  1\n",
            "--------------------------------------------------------------------------------------------\n",
            "Initializing a continuous action space policy\n",
            "--------------------------------------------------------------------------------------------\n",
            "starting std of action distribution :  1.9\n",
            "decay rate of std of action distribution :  0.02\n",
            "minimum std of action distribution :  0.05\n",
            "decay frequency of std of action distribution : 10000 timesteps\n",
            "--------------------------------------------------------------------------------------------\n",
            "PPO update frequency : 1600 timesteps\n",
            "PPO K epochs :  40\n",
            "PPO epsilon clip :  0.2\n",
            "discount factor (gamma) :  0.98\n",
            "--------------------------------------------------------------------------------------------\n",
            "optimizer learning rate actor :  0.0001\n",
            "optimizer learning rate critic :  0.0003\n",
            "============================================================================================\n",
            "Started training at (GMT) :  2024-12-11 11:51:12\n",
            "============================================================================================\n",
            "Episode : 3 \t\t Timestep : 1600 \t\t Average Reward : -0.83\n",
            "Episode : 7 \t\t Timestep : 3200 \t\t Average Reward : -0.81\n",
            "Episode : 11 \t\t Timestep : 4800 \t\t Average Reward : -0.61\n",
            "Episode : 15 \t\t Timestep : 6400 \t\t Average Reward : -0.48\n",
            "Episode : 19 \t\t Timestep : 8000 \t\t Average Reward : -0.35\n",
            "Episode : 23 \t\t Timestep : 9600 \t\t Average Reward : -0.37\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.88\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 27 \t\t Timestep : 11200 \t\t Average Reward : -0.27\n",
            "Episode : 31 \t\t Timestep : 12800 \t\t Average Reward : -0.17\n",
            "Episode : 35 \t\t Timestep : 14400 \t\t Average Reward : -0.29\n",
            "Episode : 39 \t\t Timestep : 16000 \t\t Average Reward : -0.22\n",
            "Episode : 43 \t\t Timestep : 17600 \t\t Average Reward : -0.2\n",
            "Episode : 47 \t\t Timestep : 19200 \t\t Average Reward : -0.43\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.86\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:00:36\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 51 \t\t Timestep : 20800 \t\t Average Reward : -0.21\n",
            "Episode : 55 \t\t Timestep : 22400 \t\t Average Reward : -0.1\n",
            "Episode : 59 \t\t Timestep : 24000 \t\t Average Reward : -0.25\n",
            "Episode : 63 \t\t Timestep : 25600 \t\t Average Reward : -0.19\n",
            "Episode : 67 \t\t Timestep : 27200 \t\t Average Reward : -0.09\n",
            "Episode : 71 \t\t Timestep : 28800 \t\t Average Reward : -0.07\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.84\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 75 \t\t Timestep : 30400 \t\t Average Reward : -0.28\n",
            "Episode : 79 \t\t Timestep : 32000 \t\t Average Reward : -0.15\n",
            "Episode : 83 \t\t Timestep : 33600 \t\t Average Reward : -0.28\n",
            "Episode : 87 \t\t Timestep : 35200 \t\t Average Reward : -0.13\n",
            "Episode : 91 \t\t Timestep : 36800 \t\t Average Reward : -0.17\n",
            "Episode : 95 \t\t Timestep : 38400 \t\t Average Reward : -0.12\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.82\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 99 \t\t Timestep : 40000 \t\t Average Reward : -0.13\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:01:09\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 103 \t\t Timestep : 41600 \t\t Average Reward : -0.3\n",
            "Episode : 107 \t\t Timestep : 43200 \t\t Average Reward : -0.13\n",
            "Episode : 111 \t\t Timestep : 44800 \t\t Average Reward : -0.08\n",
            "Episode : 115 \t\t Timestep : 46400 \t\t Average Reward : -0.06\n",
            "Episode : 119 \t\t Timestep : 48000 \t\t Average Reward : -0.16\n",
            "Episode : 123 \t\t Timestep : 49600 \t\t Average Reward : -0.1\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.8\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 127 \t\t Timestep : 51200 \t\t Average Reward : -0.04\n",
            "Episode : 131 \t\t Timestep : 52800 \t\t Average Reward : -0.12\n",
            "Episode : 135 \t\t Timestep : 54400 \t\t Average Reward : -0.08\n",
            "Episode : 139 \t\t Timestep : 56000 \t\t Average Reward : -0.06\n",
            "Episode : 143 \t\t Timestep : 57600 \t\t Average Reward : -0.12\n",
            "Episode : 147 \t\t Timestep : 59200 \t\t Average Reward : -0.1\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.78\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:01:43\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 151 \t\t Timestep : 60800 \t\t Average Reward : -0.09\n",
            "Episode : 155 \t\t Timestep : 62400 \t\t Average Reward : -0.12\n",
            "Episode : 159 \t\t Timestep : 64000 \t\t Average Reward : -0.11\n",
            "Episode : 163 \t\t Timestep : 65600 \t\t Average Reward : -0.14\n",
            "Episode : 167 \t\t Timestep : 67200 \t\t Average Reward : -0.13\n",
            "Episode : 171 \t\t Timestep : 68800 \t\t Average Reward : -0.04\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.76\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 175 \t\t Timestep : 70400 \t\t Average Reward : -0.1\n",
            "Episode : 179 \t\t Timestep : 72000 \t\t Average Reward : -0.19\n",
            "Episode : 183 \t\t Timestep : 73600 \t\t Average Reward : -0.08\n",
            "Episode : 187 \t\t Timestep : 75200 \t\t Average Reward : -0.03\n",
            "Episode : 191 \t\t Timestep : 76800 \t\t Average Reward : -0.03\n",
            "Episode : 195 \t\t Timestep : 78400 \t\t Average Reward : -0.06\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.74\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 199 \t\t Timestep : 80000 \t\t Average Reward : -0.02\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:02:22\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 203 \t\t Timestep : 81600 \t\t Average Reward : -0.16\n",
            "Episode : 207 \t\t Timestep : 83200 \t\t Average Reward : -0.07\n",
            "Episode : 211 \t\t Timestep : 84800 \t\t Average Reward : -0.07\n",
            "Episode : 215 \t\t Timestep : 86400 \t\t Average Reward : -0.12\n",
            "Episode : 219 \t\t Timestep : 88000 \t\t Average Reward : -0.11\n",
            "Episode : 223 \t\t Timestep : 89600 \t\t Average Reward : -0.03\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.72\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 227 \t\t Timestep : 91200 \t\t Average Reward : -0.11\n",
            "Episode : 231 \t\t Timestep : 92800 \t\t Average Reward : -0.14\n",
            "Episode : 235 \t\t Timestep : 94400 \t\t Average Reward : -0.06\n",
            "Episode : 239 \t\t Timestep : 96000 \t\t Average Reward : -0.07\n",
            "Episode : 243 \t\t Timestep : 97600 \t\t Average Reward : -0.09\n",
            "Episode : 247 \t\t Timestep : 99200 \t\t Average Reward : -0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.7\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:02:54\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 251 \t\t Timestep : 100800 \t\t Average Reward : -0.04\n",
            "Episode : 255 \t\t Timestep : 102400 \t\t Average Reward : -0.07\n",
            "Episode : 259 \t\t Timestep : 104000 \t\t Average Reward : -0.06\n",
            "Episode : 263 \t\t Timestep : 105600 \t\t Average Reward : -0.1\n",
            "Episode : 267 \t\t Timestep : 107200 \t\t Average Reward : -0.1\n",
            "Episode : 271 \t\t Timestep : 108800 \t\t Average Reward : -0.1\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.68\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 275 \t\t Timestep : 110400 \t\t Average Reward : 0.12\n",
            "Episode : 279 \t\t Timestep : 112000 \t\t Average Reward : -0.11\n",
            "Episode : 283 \t\t Timestep : 113600 \t\t Average Reward : -0.06\n",
            "Episode : 287 \t\t Timestep : 115200 \t\t Average Reward : -0.03\n",
            "Episode : 291 \t\t Timestep : 116800 \t\t Average Reward : -0.07\n",
            "Episode : 295 \t\t Timestep : 118400 \t\t Average Reward : -0.08\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.66\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 299 \t\t Timestep : 120000 \t\t Average Reward : -0.08\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:03:26\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 303 \t\t Timestep : 121600 \t\t Average Reward : -0.07\n",
            "Episode : 307 \t\t Timestep : 123200 \t\t Average Reward : -0.14\n",
            "Episode : 311 \t\t Timestep : 124800 \t\t Average Reward : -0.02\n",
            "Episode : 315 \t\t Timestep : 126400 \t\t Average Reward : -0.06\n",
            "Episode : 319 \t\t Timestep : 128000 \t\t Average Reward : -0.08\n",
            "Episode : 323 \t\t Timestep : 129600 \t\t Average Reward : -0.06\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.64\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 327 \t\t Timestep : 131200 \t\t Average Reward : -0.1\n",
            "Episode : 331 \t\t Timestep : 132800 \t\t Average Reward : -0.01\n",
            "Episode : 335 \t\t Timestep : 134400 \t\t Average Reward : -0.15\n",
            "Episode : 339 \t\t Timestep : 136000 \t\t Average Reward : -0.06\n",
            "Episode : 343 \t\t Timestep : 137600 \t\t Average Reward : -0.06\n",
            "Episode : 347 \t\t Timestep : 139200 \t\t Average Reward : -0.15\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.62\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:04:00\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 351 \t\t Timestep : 140800 \t\t Average Reward : -0.04\n",
            "Episode : 355 \t\t Timestep : 142400 \t\t Average Reward : -0.06\n",
            "Episode : 359 \t\t Timestep : 144000 \t\t Average Reward : -0.08\n",
            "Episode : 363 \t\t Timestep : 145600 \t\t Average Reward : -0.09\n",
            "Episode : 367 \t\t Timestep : 147200 \t\t Average Reward : -0.03\n",
            "Episode : 371 \t\t Timestep : 148800 \t\t Average Reward : -0.1\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.6\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 375 \t\t Timestep : 150400 \t\t Average Reward : -0.08\n",
            "Episode : 379 \t\t Timestep : 152000 \t\t Average Reward : -0.04\n",
            "Episode : 383 \t\t Timestep : 153600 \t\t Average Reward : -0.09\n",
            "Episode : 387 \t\t Timestep : 155200 \t\t Average Reward : -0.13\n",
            "Episode : 391 \t\t Timestep : 156800 \t\t Average Reward : -0.07\n",
            "Episode : 395 \t\t Timestep : 158400 \t\t Average Reward : -0.06\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.58\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 399 \t\t Timestep : 160000 \t\t Average Reward : -0.08\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:04:33\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 403 \t\t Timestep : 161600 \t\t Average Reward : -0.08\n",
            "Episode : 407 \t\t Timestep : 163200 \t\t Average Reward : -0.09\n",
            "Episode : 411 \t\t Timestep : 164800 \t\t Average Reward : -0.04\n",
            "Episode : 415 \t\t Timestep : 166400 \t\t Average Reward : -0.08\n",
            "Episode : 419 \t\t Timestep : 168000 \t\t Average Reward : -0.15\n",
            "Episode : 423 \t\t Timestep : 169600 \t\t Average Reward : -0.08\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.56\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 427 \t\t Timestep : 171200 \t\t Average Reward : -0.05\n",
            "Episode : 431 \t\t Timestep : 172800 \t\t Average Reward : -0.12\n",
            "Episode : 435 \t\t Timestep : 174400 \t\t Average Reward : -0.05\n",
            "Episode : 439 \t\t Timestep : 176000 \t\t Average Reward : -0.11\n",
            "Episode : 443 \t\t Timestep : 177600 \t\t Average Reward : -0.11\n",
            "Episode : 447 \t\t Timestep : 179200 \t\t Average Reward : 0.01\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.54\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:05:05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 451 \t\t Timestep : 180800 \t\t Average Reward : -0.06\n",
            "Episode : 455 \t\t Timestep : 182400 \t\t Average Reward : -0.07\n",
            "Episode : 459 \t\t Timestep : 184000 \t\t Average Reward : -0.07\n",
            "Episode : 463 \t\t Timestep : 185600 \t\t Average Reward : -0.1\n",
            "Episode : 467 \t\t Timestep : 187200 \t\t Average Reward : -0.07\n",
            "Episode : 471 \t\t Timestep : 188800 \t\t Average Reward : -0.06\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.52\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 475 \t\t Timestep : 190400 \t\t Average Reward : -0.1\n",
            "Episode : 479 \t\t Timestep : 192000 \t\t Average Reward : -0.02\n",
            "Episode : 483 \t\t Timestep : 193600 \t\t Average Reward : -0.1\n",
            "Episode : 487 \t\t Timestep : 195200 \t\t Average Reward : -0.04\n",
            "Episode : 491 \t\t Timestep : 196800 \t\t Average Reward : -0.04\n",
            "Episode : 495 \t\t Timestep : 198400 \t\t Average Reward : -0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.5\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 499 \t\t Timestep : 200000 \t\t Average Reward : -0.07\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:05:40\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 503 \t\t Timestep : 201600 \t\t Average Reward : 0.03\n",
            "Episode : 507 \t\t Timestep : 203200 \t\t Average Reward : -0.04\n",
            "Episode : 511 \t\t Timestep : 204800 \t\t Average Reward : -0.09\n",
            "Episode : 515 \t\t Timestep : 206400 \t\t Average Reward : -0.04\n",
            "Episode : 519 \t\t Timestep : 208000 \t\t Average Reward : -0.03\n",
            "Episode : 523 \t\t Timestep : 209600 \t\t Average Reward : -0.07\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.48\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 527 \t\t Timestep : 211200 \t\t Average Reward : 0.05\n",
            "Episode : 531 \t\t Timestep : 212800 \t\t Average Reward : -0.08\n",
            "Episode : 535 \t\t Timestep : 214400 \t\t Average Reward : -0.01\n",
            "Episode : 539 \t\t Timestep : 216000 \t\t Average Reward : -0.01\n",
            "Episode : 543 \t\t Timestep : 217600 \t\t Average Reward : -0.04\n",
            "Episode : 547 \t\t Timestep : 219200 \t\t Average Reward : -0.11\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.46\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:06:12\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 551 \t\t Timestep : 220800 \t\t Average Reward : 0.0\n",
            "Episode : 555 \t\t Timestep : 222400 \t\t Average Reward : -0.07\n",
            "Episode : 559 \t\t Timestep : 224000 \t\t Average Reward : -0.0\n",
            "Episode : 563 \t\t Timestep : 225600 \t\t Average Reward : -0.02\n",
            "Episode : 567 \t\t Timestep : 227200 \t\t Average Reward : -0.08\n",
            "Episode : 571 \t\t Timestep : 228800 \t\t Average Reward : -0.04\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.44\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 575 \t\t Timestep : 230400 \t\t Average Reward : -0.06\n",
            "Episode : 579 \t\t Timestep : 232000 \t\t Average Reward : -0.03\n",
            "Episode : 583 \t\t Timestep : 233600 \t\t Average Reward : -0.1\n",
            "Episode : 587 \t\t Timestep : 235200 \t\t Average Reward : -0.04\n",
            "Episode : 591 \t\t Timestep : 236800 \t\t Average Reward : -0.07\n",
            "Episode : 595 \t\t Timestep : 238400 \t\t Average Reward : -0.06\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.42\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 599 \t\t Timestep : 240000 \t\t Average Reward : -0.07\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:06:47\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 603 \t\t Timestep : 241600 \t\t Average Reward : -0.07\n",
            "Episode : 607 \t\t Timestep : 243200 \t\t Average Reward : -0.11\n",
            "Episode : 611 \t\t Timestep : 244800 \t\t Average Reward : -0.12\n",
            "Episode : 615 \t\t Timestep : 246400 \t\t Average Reward : -0.01\n",
            "Episode : 619 \t\t Timestep : 248000 \t\t Average Reward : -0.01\n",
            "Episode : 623 \t\t Timestep : 249600 \t\t Average Reward : -0.04\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.4\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 627 \t\t Timestep : 251200 \t\t Average Reward : -0.13\n",
            "Episode : 631 \t\t Timestep : 252800 \t\t Average Reward : -0.03\n",
            "Episode : 635 \t\t Timestep : 254400 \t\t Average Reward : -0.0\n",
            "Episode : 639 \t\t Timestep : 256000 \t\t Average Reward : -0.07\n",
            "Episode : 643 \t\t Timestep : 257600 \t\t Average Reward : -0.06\n",
            "Episode : 647 \t\t Timestep : 259200 \t\t Average Reward : 0.03\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.38\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:07:19\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 651 \t\t Timestep : 260800 \t\t Average Reward : -0.01\n",
            "Episode : 655 \t\t Timestep : 262400 \t\t Average Reward : -0.0\n",
            "Episode : 659 \t\t Timestep : 264000 \t\t Average Reward : -0.07\n",
            "Episode : 663 \t\t Timestep : 265600 \t\t Average Reward : -0.08\n",
            "Episode : 667 \t\t Timestep : 267200 \t\t Average Reward : -0.0\n",
            "Episode : 671 \t\t Timestep : 268800 \t\t Average Reward : -0.03\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.36\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 675 \t\t Timestep : 270400 \t\t Average Reward : 0.01\n",
            "Episode : 679 \t\t Timestep : 272000 \t\t Average Reward : -0.1\n",
            "Episode : 683 \t\t Timestep : 273600 \t\t Average Reward : -0.08\n",
            "Episode : 687 \t\t Timestep : 275200 \t\t Average Reward : -0.1\n",
            "Episode : 691 \t\t Timestep : 276800 \t\t Average Reward : -0.03\n",
            "Episode : 695 \t\t Timestep : 278400 \t\t Average Reward : -0.14\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.34\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 699 \t\t Timestep : 280000 \t\t Average Reward : -0.08\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:07:57\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 703 \t\t Timestep : 281600 \t\t Average Reward : -0.08\n",
            "Episode : 707 \t\t Timestep : 283200 \t\t Average Reward : -0.07\n",
            "Episode : 711 \t\t Timestep : 284800 \t\t Average Reward : -0.04\n",
            "Episode : 715 \t\t Timestep : 286400 \t\t Average Reward : -0.03\n",
            "Episode : 719 \t\t Timestep : 288000 \t\t Average Reward : -0.03\n",
            "Episode : 723 \t\t Timestep : 289600 \t\t Average Reward : -0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.32\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 727 \t\t Timestep : 291200 \t\t Average Reward : -0.13\n",
            "Episode : 731 \t\t Timestep : 292800 \t\t Average Reward : -0.04\n",
            "Episode : 735 \t\t Timestep : 294400 \t\t Average Reward : -0.02\n",
            "Episode : 739 \t\t Timestep : 296000 \t\t Average Reward : -0.1\n",
            "Episode : 743 \t\t Timestep : 297600 \t\t Average Reward : -0.19\n",
            "Episode : 747 \t\t Timestep : 299200 \t\t Average Reward : 0.03\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:08:33\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 751 \t\t Timestep : 300800 \t\t Average Reward : -0.09\n",
            "Episode : 755 \t\t Timestep : 302400 \t\t Average Reward : -0.06\n",
            "Episode : 759 \t\t Timestep : 304000 \t\t Average Reward : -0.03\n",
            "Episode : 763 \t\t Timestep : 305600 \t\t Average Reward : 0.02\n",
            "Episode : 767 \t\t Timestep : 307200 \t\t Average Reward : 0.06\n",
            "Episode : 771 \t\t Timestep : 308800 \t\t Average Reward : -0.12\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.28\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 775 \t\t Timestep : 310400 \t\t Average Reward : -0.08\n",
            "Episode : 779 \t\t Timestep : 312000 \t\t Average Reward : -0.16\n",
            "Episode : 783 \t\t Timestep : 313600 \t\t Average Reward : -0.07\n",
            "Episode : 787 \t\t Timestep : 315200 \t\t Average Reward : -0.0\n",
            "Episode : 791 \t\t Timestep : 316800 \t\t Average Reward : -0.04\n",
            "Episode : 795 \t\t Timestep : 318400 \t\t Average Reward : -0.01\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.26\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 799 \t\t Timestep : 320000 \t\t Average Reward : -0.01\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:09:05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 803 \t\t Timestep : 321600 \t\t Average Reward : -0.05\n",
            "Episode : 807 \t\t Timestep : 323200 \t\t Average Reward : -0.09\n",
            "Episode : 811 \t\t Timestep : 324800 \t\t Average Reward : -0.05\n",
            "Episode : 815 \t\t Timestep : 326400 \t\t Average Reward : -0.06\n",
            "Episode : 819 \t\t Timestep : 328000 \t\t Average Reward : -0.1\n",
            "Episode : 823 \t\t Timestep : 329600 \t\t Average Reward : 0.01\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.24\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 827 \t\t Timestep : 331200 \t\t Average Reward : -0.01\n",
            "Episode : 831 \t\t Timestep : 332800 \t\t Average Reward : 0.09\n",
            "Episode : 835 \t\t Timestep : 334400 \t\t Average Reward : -0.05\n",
            "Episode : 839 \t\t Timestep : 336000 \t\t Average Reward : -0.03\n",
            "Episode : 843 \t\t Timestep : 337600 \t\t Average Reward : -0.08\n",
            "Episode : 847 \t\t Timestep : 339200 \t\t Average Reward : -0.06\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.22\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:09:38\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 851 \t\t Timestep : 340800 \t\t Average Reward : -0.04\n",
            "Episode : 855 \t\t Timestep : 342400 \t\t Average Reward : -0.1\n",
            "Episode : 859 \t\t Timestep : 344000 \t\t Average Reward : -0.06\n",
            "Episode : 863 \t\t Timestep : 345600 \t\t Average Reward : -0.08\n",
            "Episode : 867 \t\t Timestep : 347200 \t\t Average Reward : -0.11\n",
            "Episode : 871 \t\t Timestep : 348800 \t\t Average Reward : -0.06\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.2\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 875 \t\t Timestep : 350400 \t\t Average Reward : 0.01\n",
            "Episode : 879 \t\t Timestep : 352000 \t\t Average Reward : -0.09\n",
            "Episode : 883 \t\t Timestep : 353600 \t\t Average Reward : -0.05\n",
            "Episode : 887 \t\t Timestep : 355200 \t\t Average Reward : -0.1\n",
            "Episode : 891 \t\t Timestep : 356800 \t\t Average Reward : -0.07\n",
            "Episode : 895 \t\t Timestep : 358400 \t\t Average Reward : 0.03\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.18\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 899 \t\t Timestep : 360000 \t\t Average Reward : -0.0\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:10:11\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 903 \t\t Timestep : 361600 \t\t Average Reward : -0.07\n",
            "Episode : 907 \t\t Timestep : 363200 \t\t Average Reward : -0.06\n",
            "Episode : 911 \t\t Timestep : 364800 \t\t Average Reward : -0.08\n",
            "Episode : 915 \t\t Timestep : 366400 \t\t Average Reward : 0.01\n",
            "Episode : 919 \t\t Timestep : 368000 \t\t Average Reward : 0.1\n",
            "Episode : 923 \t\t Timestep : 369600 \t\t Average Reward : -0.02\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.16\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 927 \t\t Timestep : 371200 \t\t Average Reward : -0.0\n",
            "Episode : 931 \t\t Timestep : 372800 \t\t Average Reward : -0.02\n",
            "Episode : 935 \t\t Timestep : 374400 \t\t Average Reward : -0.0\n",
            "Episode : 939 \t\t Timestep : 376000 \t\t Average Reward : 0.05\n",
            "Episode : 943 \t\t Timestep : 377600 \t\t Average Reward : -0.02\n",
            "Episode : 947 \t\t Timestep : 379200 \t\t Average Reward : 0.03\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.14\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:10:44\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 951 \t\t Timestep : 380800 \t\t Average Reward : 0.02\n",
            "Episode : 955 \t\t Timestep : 382400 \t\t Average Reward : -0.07\n",
            "Episode : 959 \t\t Timestep : 384000 \t\t Average Reward : 0.08\n",
            "Episode : 963 \t\t Timestep : 385600 \t\t Average Reward : -0.08\n",
            "Episode : 967 \t\t Timestep : 387200 \t\t Average Reward : -0.06\n",
            "Episode : 971 \t\t Timestep : 388800 \t\t Average Reward : 0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.12\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 975 \t\t Timestep : 390400 \t\t Average Reward : -0.05\n",
            "Episode : 979 \t\t Timestep : 392000 \t\t Average Reward : -0.07\n",
            "Episode : 983 \t\t Timestep : 393600 \t\t Average Reward : -0.07\n",
            "Episode : 987 \t\t Timestep : 395200 \t\t Average Reward : 0.01\n",
            "Episode : 991 \t\t Timestep : 396800 \t\t Average Reward : -0.03\n",
            "Episode : 995 \t\t Timestep : 398400 \t\t Average Reward : -0.02\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.1\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 999 \t\t Timestep : 400000 \t\t Average Reward : 0.04\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:11:17\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1003 \t\t Timestep : 401600 \t\t Average Reward : -0.08\n",
            "Episode : 1007 \t\t Timestep : 403200 \t\t Average Reward : -0.04\n",
            "Episode : 1011 \t\t Timestep : 404800 \t\t Average Reward : -0.09\n",
            "Episode : 1015 \t\t Timestep : 406400 \t\t Average Reward : -0.03\n",
            "Episode : 1019 \t\t Timestep : 408000 \t\t Average Reward : -0.13\n",
            "Episode : 1023 \t\t Timestep : 409600 \t\t Average Reward : 0.08\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.08\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1027 \t\t Timestep : 411200 \t\t Average Reward : -0.05\n",
            "Episode : 1031 \t\t Timestep : 412800 \t\t Average Reward : -0.02\n",
            "Episode : 1035 \t\t Timestep : 414400 \t\t Average Reward : 0.08\n",
            "Episode : 1039 \t\t Timestep : 416000 \t\t Average Reward : -0.04\n",
            "Episode : 1043 \t\t Timestep : 417600 \t\t Average Reward : -0.05\n",
            "Episode : 1047 \t\t Timestep : 419200 \t\t Average Reward : -0.01\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.06\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:11:51\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1051 \t\t Timestep : 420800 \t\t Average Reward : -0.02\n",
            "Episode : 1055 \t\t Timestep : 422400 \t\t Average Reward : 0.12\n",
            "Episode : 1059 \t\t Timestep : 424000 \t\t Average Reward : -0.03\n",
            "Episode : 1063 \t\t Timestep : 425600 \t\t Average Reward : 0.03\n",
            "Episode : 1067 \t\t Timestep : 427200 \t\t Average Reward : 0.0\n",
            "Episode : 1071 \t\t Timestep : 428800 \t\t Average Reward : -0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.04\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1075 \t\t Timestep : 430400 \t\t Average Reward : 0.07\n",
            "Episode : 1079 \t\t Timestep : 432000 \t\t Average Reward : -0.03\n",
            "Episode : 1083 \t\t Timestep : 433600 \t\t Average Reward : -0.02\n",
            "Episode : 1087 \t\t Timestep : 435200 \t\t Average Reward : 0.12\n",
            "Episode : 1091 \t\t Timestep : 436800 \t\t Average Reward : -0.03\n",
            "Episode : 1095 \t\t Timestep : 438400 \t\t Average Reward : 0.07\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.02\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1099 \t\t Timestep : 440000 \t\t Average Reward : -0.04\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:12:27\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1103 \t\t Timestep : 441600 \t\t Average Reward : -0.0\n",
            "Episode : 1107 \t\t Timestep : 443200 \t\t Average Reward : 0.09\n",
            "Episode : 1111 \t\t Timestep : 444800 \t\t Average Reward : 0.03\n",
            "Episode : 1115 \t\t Timestep : 446400 \t\t Average Reward : -0.01\n",
            "Episode : 1119 \t\t Timestep : 448000 \t\t Average Reward : 0.05\n",
            "Episode : 1123 \t\t Timestep : 449600 \t\t Average Reward : -0.02\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  1.0\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1127 \t\t Timestep : 451200 \t\t Average Reward : 0.02\n",
            "Episode : 1131 \t\t Timestep : 452800 \t\t Average Reward : -0.01\n",
            "Episode : 1135 \t\t Timestep : 454400 \t\t Average Reward : 0.06\n",
            "Episode : 1139 \t\t Timestep : 456000 \t\t Average Reward : 0.05\n",
            "Episode : 1143 \t\t Timestep : 457600 \t\t Average Reward : -0.01\n",
            "Episode : 1147 \t\t Timestep : 459200 \t\t Average Reward : -0.02\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.98\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:12:59\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1151 \t\t Timestep : 460800 \t\t Average Reward : -0.04\n",
            "Episode : 1155 \t\t Timestep : 462400 \t\t Average Reward : 0.04\n",
            "Episode : 1159 \t\t Timestep : 464000 \t\t Average Reward : -0.0\n",
            "Episode : 1163 \t\t Timestep : 465600 \t\t Average Reward : 0.06\n",
            "Episode : 1167 \t\t Timestep : 467200 \t\t Average Reward : 0.03\n",
            "Episode : 1171 \t\t Timestep : 468800 \t\t Average Reward : -0.04\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.96\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1175 \t\t Timestep : 470400 \t\t Average Reward : 0.02\n",
            "Episode : 1179 \t\t Timestep : 472000 \t\t Average Reward : 0.06\n",
            "Episode : 1183 \t\t Timestep : 473600 \t\t Average Reward : 0.14\n",
            "Episode : 1187 \t\t Timestep : 475200 \t\t Average Reward : 0.0\n",
            "Episode : 1191 \t\t Timestep : 476800 \t\t Average Reward : 0.1\n",
            "Episode : 1195 \t\t Timestep : 478400 \t\t Average Reward : 0.06\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.94\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1199 \t\t Timestep : 480000 \t\t Average Reward : 0.15\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:13:32\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1203 \t\t Timestep : 481600 \t\t Average Reward : 0.1\n",
            "Episode : 1207 \t\t Timestep : 483200 \t\t Average Reward : 0.09\n",
            "Episode : 1211 \t\t Timestep : 484800 \t\t Average Reward : 0.09\n",
            "Episode : 1215 \t\t Timestep : 486400 \t\t Average Reward : 0.09\n",
            "Episode : 1219 \t\t Timestep : 488000 \t\t Average Reward : 0.11\n",
            "Episode : 1223 \t\t Timestep : 489600 \t\t Average Reward : 0.06\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.92\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1227 \t\t Timestep : 491200 \t\t Average Reward : 0.03\n",
            "Episode : 1231 \t\t Timestep : 492800 \t\t Average Reward : -0.01\n",
            "Episode : 1235 \t\t Timestep : 494400 \t\t Average Reward : 0.03\n",
            "Episode : 1239 \t\t Timestep : 496000 \t\t Average Reward : 0.07\n",
            "Episode : 1243 \t\t Timestep : 497600 \t\t Average Reward : 0.04\n",
            "Episode : 1247 \t\t Timestep : 499200 \t\t Average Reward : -0.0\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.9\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:14:03\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1251 \t\t Timestep : 500800 \t\t Average Reward : 0.03\n",
            "Episode : 1255 \t\t Timestep : 502400 \t\t Average Reward : 0.16\n",
            "Episode : 1259 \t\t Timestep : 504000 \t\t Average Reward : 0.07\n",
            "Episode : 1263 \t\t Timestep : 505600 \t\t Average Reward : 0.02\n",
            "Episode : 1267 \t\t Timestep : 507200 \t\t Average Reward : 0.01\n",
            "Episode : 1271 \t\t Timestep : 508800 \t\t Average Reward : 0.15\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.88\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1275 \t\t Timestep : 510400 \t\t Average Reward : 0.04\n",
            "Episode : 1279 \t\t Timestep : 512000 \t\t Average Reward : 0.1\n",
            "Episode : 1283 \t\t Timestep : 513600 \t\t Average Reward : -0.01\n",
            "Episode : 1287 \t\t Timestep : 515200 \t\t Average Reward : 0.12\n",
            "Episode : 1291 \t\t Timestep : 516800 \t\t Average Reward : 0.01\n",
            "Episode : 1295 \t\t Timestep : 518400 \t\t Average Reward : 0.15\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.86\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1299 \t\t Timestep : 520000 \t\t Average Reward : -0.03\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:14:36\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1303 \t\t Timestep : 521600 \t\t Average Reward : 0.11\n",
            "Episode : 1307 \t\t Timestep : 523200 \t\t Average Reward : 0.08\n",
            "Episode : 1311 \t\t Timestep : 524800 \t\t Average Reward : 0.05\n",
            "Episode : 1315 \t\t Timestep : 526400 \t\t Average Reward : 0.09\n",
            "Episode : 1319 \t\t Timestep : 528000 \t\t Average Reward : 0.02\n",
            "Episode : 1323 \t\t Timestep : 529600 \t\t Average Reward : 0.04\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.84\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1327 \t\t Timestep : 531200 \t\t Average Reward : 0.12\n",
            "Episode : 1331 \t\t Timestep : 532800 \t\t Average Reward : 0.09\n",
            "Episode : 1335 \t\t Timestep : 534400 \t\t Average Reward : -0.0\n",
            "Episode : 1339 \t\t Timestep : 536000 \t\t Average Reward : 0.05\n",
            "Episode : 1343 \t\t Timestep : 537600 \t\t Average Reward : 0.01\n",
            "Episode : 1347 \t\t Timestep : 539200 \t\t Average Reward : 0.1\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.82\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:15:08\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1351 \t\t Timestep : 540800 \t\t Average Reward : 0.11\n",
            "Episode : 1355 \t\t Timestep : 542400 \t\t Average Reward : -0.02\n",
            "Episode : 1359 \t\t Timestep : 544000 \t\t Average Reward : 0.08\n",
            "Episode : 1363 \t\t Timestep : 545600 \t\t Average Reward : 0.05\n",
            "Episode : 1367 \t\t Timestep : 547200 \t\t Average Reward : 0.18\n",
            "Episode : 1371 \t\t Timestep : 548800 \t\t Average Reward : 0.17\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.8\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1375 \t\t Timestep : 550400 \t\t Average Reward : 0.14\n",
            "Episode : 1379 \t\t Timestep : 552000 \t\t Average Reward : 0.06\n",
            "Episode : 1383 \t\t Timestep : 553600 \t\t Average Reward : 0.1\n",
            "Episode : 1387 \t\t Timestep : 555200 \t\t Average Reward : 0.07\n",
            "Episode : 1391 \t\t Timestep : 556800 \t\t Average Reward : 0.08\n",
            "Episode : 1395 \t\t Timestep : 558400 \t\t Average Reward : 0.09\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.78\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1399 \t\t Timestep : 560000 \t\t Average Reward : 0.01\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:15:41\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1403 \t\t Timestep : 561600 \t\t Average Reward : 0.13\n",
            "Episode : 1407 \t\t Timestep : 563200 \t\t Average Reward : 0.08\n",
            "Episode : 1411 \t\t Timestep : 564800 \t\t Average Reward : 0.15\n",
            "Episode : 1415 \t\t Timestep : 566400 \t\t Average Reward : 0.04\n",
            "Episode : 1419 \t\t Timestep : 568000 \t\t Average Reward : 0.05\n",
            "Episode : 1423 \t\t Timestep : 569600 \t\t Average Reward : 0.09\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.76\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1427 \t\t Timestep : 571200 \t\t Average Reward : 0.07\n",
            "Episode : 1431 \t\t Timestep : 572800 \t\t Average Reward : 0.17\n",
            "Episode : 1435 \t\t Timestep : 574400 \t\t Average Reward : 0.07\n",
            "Episode : 1439 \t\t Timestep : 576000 \t\t Average Reward : 0.16\n",
            "Episode : 1443 \t\t Timestep : 577600 \t\t Average Reward : 0.04\n",
            "Episode : 1447 \t\t Timestep : 579200 \t\t Average Reward : 0.15\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.74\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:16:12\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1451 \t\t Timestep : 580800 \t\t Average Reward : 0.09\n",
            "Episode : 1455 \t\t Timestep : 582400 \t\t Average Reward : 0.1\n",
            "Episode : 1459 \t\t Timestep : 584000 \t\t Average Reward : 0.08\n",
            "Episode : 1463 \t\t Timestep : 585600 \t\t Average Reward : 0.12\n",
            "Episode : 1467 \t\t Timestep : 587200 \t\t Average Reward : 0.09\n",
            "Episode : 1471 \t\t Timestep : 588800 \t\t Average Reward : 0.17\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.72\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1475 \t\t Timestep : 590400 \t\t Average Reward : 0.09\n",
            "Episode : 1479 \t\t Timestep : 592000 \t\t Average Reward : 0.06\n",
            "Episode : 1483 \t\t Timestep : 593600 \t\t Average Reward : 0.11\n",
            "Episode : 1487 \t\t Timestep : 595200 \t\t Average Reward : 0.22\n",
            "Episode : 1491 \t\t Timestep : 596800 \t\t Average Reward : 0.04\n",
            "Episode : 1495 \t\t Timestep : 598400 \t\t Average Reward : 0.35\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.7\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1499 \t\t Timestep : 600000 \t\t Average Reward : 0.11\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:16:46\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1503 \t\t Timestep : 601600 \t\t Average Reward : 0.09\n",
            "Episode : 1507 \t\t Timestep : 603200 \t\t Average Reward : 0.06\n",
            "Episode : 1511 \t\t Timestep : 604800 \t\t Average Reward : 0.13\n",
            "Episode : 1515 \t\t Timestep : 606400 \t\t Average Reward : 0.12\n",
            "Episode : 1519 \t\t Timestep : 608000 \t\t Average Reward : 0.12\n",
            "Episode : 1523 \t\t Timestep : 609600 \t\t Average Reward : 0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.68\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1527 \t\t Timestep : 611200 \t\t Average Reward : 0.12\n",
            "Episode : 1531 \t\t Timestep : 612800 \t\t Average Reward : 0.13\n",
            "Episode : 1535 \t\t Timestep : 614400 \t\t Average Reward : 0.15\n",
            "Episode : 1539 \t\t Timestep : 616000 \t\t Average Reward : 0.05\n",
            "Episode : 1543 \t\t Timestep : 617600 \t\t Average Reward : 0.07\n",
            "Episode : 1547 \t\t Timestep : 619200 \t\t Average Reward : 0.21\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.66\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:17:18\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1551 \t\t Timestep : 620800 \t\t Average Reward : 0.14\n",
            "Episode : 1555 \t\t Timestep : 622400 \t\t Average Reward : 0.09\n",
            "Episode : 1559 \t\t Timestep : 624000 \t\t Average Reward : 0.18\n",
            "Episode : 1563 \t\t Timestep : 625600 \t\t Average Reward : 0.21\n",
            "Episode : 1567 \t\t Timestep : 627200 \t\t Average Reward : 0.16\n",
            "Episode : 1571 \t\t Timestep : 628800 \t\t Average Reward : 0.14\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.64\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1575 \t\t Timestep : 630400 \t\t Average Reward : 0.3\n",
            "Episode : 1579 \t\t Timestep : 632000 \t\t Average Reward : 0.16\n",
            "Episode : 1583 \t\t Timestep : 633600 \t\t Average Reward : 0.16\n",
            "Episode : 1587 \t\t Timestep : 635200 \t\t Average Reward : 0.13\n",
            "Episode : 1591 \t\t Timestep : 636800 \t\t Average Reward : 0.17\n",
            "Episode : 1595 \t\t Timestep : 638400 \t\t Average Reward : 0.19\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.62\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1599 \t\t Timestep : 640000 \t\t Average Reward : 0.08\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:17:51\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1603 \t\t Timestep : 641600 \t\t Average Reward : 0.19\n",
            "Episode : 1607 \t\t Timestep : 643200 \t\t Average Reward : 0.13\n",
            "Episode : 1611 \t\t Timestep : 644800 \t\t Average Reward : 0.17\n",
            "Episode : 1615 \t\t Timestep : 646400 \t\t Average Reward : 0.24\n",
            "Episode : 1619 \t\t Timestep : 648000 \t\t Average Reward : 0.22\n",
            "Episode : 1623 \t\t Timestep : 649600 \t\t Average Reward : 0.23\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.6\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1627 \t\t Timestep : 651200 \t\t Average Reward : 0.16\n",
            "Episode : 1631 \t\t Timestep : 652800 \t\t Average Reward : 0.15\n",
            "Episode : 1635 \t\t Timestep : 654400 \t\t Average Reward : 0.23\n",
            "Episode : 1639 \t\t Timestep : 656000 \t\t Average Reward : 0.17\n",
            "Episode : 1643 \t\t Timestep : 657600 \t\t Average Reward : 0.2\n",
            "Episode : 1647 \t\t Timestep : 659200 \t\t Average Reward : 0.14\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.58\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:18:24\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1651 \t\t Timestep : 660800 \t\t Average Reward : 0.22\n",
            "Episode : 1655 \t\t Timestep : 662400 \t\t Average Reward : 0.22\n",
            "Episode : 1659 \t\t Timestep : 664000 \t\t Average Reward : 0.17\n",
            "Episode : 1663 \t\t Timestep : 665600 \t\t Average Reward : 0.39\n",
            "Episode : 1667 \t\t Timestep : 667200 \t\t Average Reward : 0.13\n",
            "Episode : 1671 \t\t Timestep : 668800 \t\t Average Reward : 0.17\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.56\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1675 \t\t Timestep : 670400 \t\t Average Reward : 0.07\n",
            "Episode : 1679 \t\t Timestep : 672000 \t\t Average Reward : 0.01\n",
            "Episode : 1683 \t\t Timestep : 673600 \t\t Average Reward : 0.15\n",
            "Episode : 1687 \t\t Timestep : 675200 \t\t Average Reward : 0.23\n",
            "Episode : 1691 \t\t Timestep : 676800 \t\t Average Reward : 0.12\n",
            "Episode : 1695 \t\t Timestep : 678400 \t\t Average Reward : 0.19\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.54\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1699 \t\t Timestep : 680000 \t\t Average Reward : -0.02\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:18:57\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1703 \t\t Timestep : 681600 \t\t Average Reward : 0.12\n",
            "Episode : 1707 \t\t Timestep : 683200 \t\t Average Reward : 0.09\n",
            "Episode : 1711 \t\t Timestep : 684800 \t\t Average Reward : 0.28\n",
            "Episode : 1715 \t\t Timestep : 686400 \t\t Average Reward : 0.19\n",
            "Episode : 1719 \t\t Timestep : 688000 \t\t Average Reward : 0.18\n",
            "Episode : 1723 \t\t Timestep : 689600 \t\t Average Reward : 0.17\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.52\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1727 \t\t Timestep : 691200 \t\t Average Reward : 0.12\n",
            "Episode : 1731 \t\t Timestep : 692800 \t\t Average Reward : 0.21\n",
            "Episode : 1735 \t\t Timestep : 694400 \t\t Average Reward : 0.08\n",
            "Episode : 1739 \t\t Timestep : 696000 \t\t Average Reward : 0.12\n",
            "Episode : 1743 \t\t Timestep : 697600 \t\t Average Reward : 0.36\n",
            "Episode : 1747 \t\t Timestep : 699200 \t\t Average Reward : 0.1\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.5\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:19:28\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1751 \t\t Timestep : 700800 \t\t Average Reward : 0.17\n",
            "Episode : 1755 \t\t Timestep : 702400 \t\t Average Reward : 0.15\n",
            "Episode : 1759 \t\t Timestep : 704000 \t\t Average Reward : 0.19\n",
            "Episode : 1763 \t\t Timestep : 705600 \t\t Average Reward : 0.12\n",
            "Episode : 1767 \t\t Timestep : 707200 \t\t Average Reward : 0.22\n",
            "Episode : 1771 \t\t Timestep : 708800 \t\t Average Reward : 0.29\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.48\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1775 \t\t Timestep : 710400 \t\t Average Reward : 0.23\n",
            "Episode : 1779 \t\t Timestep : 712000 \t\t Average Reward : 0.33\n",
            "Episode : 1783 \t\t Timestep : 713600 \t\t Average Reward : 0.06\n",
            "Episode : 1787 \t\t Timestep : 715200 \t\t Average Reward : 0.21\n",
            "Episode : 1791 \t\t Timestep : 716800 \t\t Average Reward : 0.26\n",
            "Episode : 1795 \t\t Timestep : 718400 \t\t Average Reward : 0.34\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.46\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1799 \t\t Timestep : 720000 \t\t Average Reward : 0.37\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:20:01\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1803 \t\t Timestep : 721600 \t\t Average Reward : 0.14\n",
            "Episode : 1807 \t\t Timestep : 723200 \t\t Average Reward : 0.22\n",
            "Episode : 1811 \t\t Timestep : 724800 \t\t Average Reward : 0.17\n",
            "Episode : 1815 \t\t Timestep : 726400 \t\t Average Reward : 0.27\n",
            "Episode : 1819 \t\t Timestep : 728000 \t\t Average Reward : 0.28\n",
            "Episode : 1823 \t\t Timestep : 729600 \t\t Average Reward : 0.21\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.44\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1827 \t\t Timestep : 731200 \t\t Average Reward : 0.3\n",
            "Episode : 1831 \t\t Timestep : 732800 \t\t Average Reward : 0.2\n",
            "Episode : 1835 \t\t Timestep : 734400 \t\t Average Reward : 0.21\n",
            "Episode : 1839 \t\t Timestep : 736000 \t\t Average Reward : 0.33\n",
            "Episode : 1843 \t\t Timestep : 737600 \t\t Average Reward : 0.15\n",
            "Episode : 1847 \t\t Timestep : 739200 \t\t Average Reward : -0.12\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.42\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:20:32\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1851 \t\t Timestep : 740800 \t\t Average Reward : 0.32\n",
            "Episode : 1855 \t\t Timestep : 742400 \t\t Average Reward : 0.14\n",
            "Episode : 1859 \t\t Timestep : 744000 \t\t Average Reward : 0.25\n",
            "Episode : 1863 \t\t Timestep : 745600 \t\t Average Reward : 0.06\n",
            "Episode : 1867 \t\t Timestep : 747200 \t\t Average Reward : 0.36\n",
            "Episode : 1871 \t\t Timestep : 748800 \t\t Average Reward : 0.36\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.4\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1875 \t\t Timestep : 750400 \t\t Average Reward : 0.25\n",
            "Episode : 1879 \t\t Timestep : 752000 \t\t Average Reward : 0.21\n",
            "Episode : 1883 \t\t Timestep : 753600 \t\t Average Reward : 0.31\n",
            "Episode : 1887 \t\t Timestep : 755200 \t\t Average Reward : 0.22\n",
            "Episode : 1891 \t\t Timestep : 756800 \t\t Average Reward : 0.37\n",
            "Episode : 1895 \t\t Timestep : 758400 \t\t Average Reward : 0.29\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.38\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1899 \t\t Timestep : 760000 \t\t Average Reward : 0.44\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:21:04\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1903 \t\t Timestep : 761600 \t\t Average Reward : 0.38\n",
            "Episode : 1907 \t\t Timestep : 763200 \t\t Average Reward : 0.02\n",
            "Episode : 1911 \t\t Timestep : 764800 \t\t Average Reward : 0.25\n",
            "Episode : 1915 \t\t Timestep : 766400 \t\t Average Reward : 0.21\n",
            "Episode : 1919 \t\t Timestep : 768000 \t\t Average Reward : 0.34\n",
            "Episode : 1923 \t\t Timestep : 769600 \t\t Average Reward : 0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.36\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1927 \t\t Timestep : 771200 \t\t Average Reward : 0.38\n",
            "Episode : 1931 \t\t Timestep : 772800 \t\t Average Reward : 0.32\n",
            "Episode : 1935 \t\t Timestep : 774400 \t\t Average Reward : 0.48\n",
            "Episode : 1939 \t\t Timestep : 776000 \t\t Average Reward : 0.21\n",
            "Episode : 1943 \t\t Timestep : 777600 \t\t Average Reward : 0.27\n",
            "Episode : 1947 \t\t Timestep : 779200 \t\t Average Reward : -0.08\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.34\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:21:37\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1951 \t\t Timestep : 780800 \t\t Average Reward : -0.1\n",
            "Episode : 1955 \t\t Timestep : 782400 \t\t Average Reward : -0.07\n",
            "Episode : 1959 \t\t Timestep : 784000 \t\t Average Reward : 0.27\n",
            "Episode : 1963 \t\t Timestep : 785600 \t\t Average Reward : 0.24\n",
            "Episode : 1967 \t\t Timestep : 787200 \t\t Average Reward : 0.29\n",
            "Episode : 1971 \t\t Timestep : 788800 \t\t Average Reward : 0.26\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.32\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1975 \t\t Timestep : 790400 \t\t Average Reward : 0.22\n",
            "Episode : 1979 \t\t Timestep : 792000 \t\t Average Reward : 0.39\n",
            "Episode : 1983 \t\t Timestep : 793600 \t\t Average Reward : 0.37\n",
            "Episode : 1987 \t\t Timestep : 795200 \t\t Average Reward : 0.32\n",
            "Episode : 1991 \t\t Timestep : 796800 \t\t Average Reward : 0.2\n",
            "Episode : 1995 \t\t Timestep : 798400 \t\t Average Reward : 0.32\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 1999 \t\t Timestep : 800000 \t\t Average Reward : 0.24\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:22:09\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2003 \t\t Timestep : 801600 \t\t Average Reward : 0.28\n",
            "Episode : 2007 \t\t Timestep : 803200 \t\t Average Reward : 0.33\n",
            "Episode : 2011 \t\t Timestep : 804800 \t\t Average Reward : 0.28\n",
            "Episode : 2015 \t\t Timestep : 806400 \t\t Average Reward : 0.39\n",
            "Episode : 2019 \t\t Timestep : 808000 \t\t Average Reward : 0.28\n",
            "Episode : 2023 \t\t Timestep : 809600 \t\t Average Reward : 0.31\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.28\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2027 \t\t Timestep : 811200 \t\t Average Reward : 0.29\n",
            "Episode : 2031 \t\t Timestep : 812800 \t\t Average Reward : 0.3\n",
            "Episode : 2035 \t\t Timestep : 814400 \t\t Average Reward : 0.33\n",
            "Episode : 2039 \t\t Timestep : 816000 \t\t Average Reward : 0.29\n",
            "Episode : 2043 \t\t Timestep : 817600 \t\t Average Reward : 0.2\n",
            "Episode : 2047 \t\t Timestep : 819200 \t\t Average Reward : 0.18\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.26\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:22:40\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2051 \t\t Timestep : 820800 \t\t Average Reward : 0.3\n",
            "Episode : 2055 \t\t Timestep : 822400 \t\t Average Reward : 0.43\n",
            "Episode : 2059 \t\t Timestep : 824000 \t\t Average Reward : 0.4\n",
            "Episode : 2063 \t\t Timestep : 825600 \t\t Average Reward : 0.48\n",
            "Episode : 2067 \t\t Timestep : 827200 \t\t Average Reward : 0.31\n",
            "Episode : 2071 \t\t Timestep : 828800 \t\t Average Reward : 0.31\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.24\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2075 \t\t Timestep : 830400 \t\t Average Reward : 0.32\n",
            "Episode : 2079 \t\t Timestep : 832000 \t\t Average Reward : 0.43\n",
            "Episode : 2083 \t\t Timestep : 833600 \t\t Average Reward : 0.45\n",
            "Episode : 2087 \t\t Timestep : 835200 \t\t Average Reward : 0.31\n",
            "Episode : 2091 \t\t Timestep : 836800 \t\t Average Reward : 0.26\n",
            "Episode : 2095 \t\t Timestep : 838400 \t\t Average Reward : 0.36\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.22\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2099 \t\t Timestep : 840000 \t\t Average Reward : 0.39\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:23:12\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2103 \t\t Timestep : 841600 \t\t Average Reward : 0.36\n",
            "Episode : 2107 \t\t Timestep : 843200 \t\t Average Reward : 0.47\n",
            "Episode : 2111 \t\t Timestep : 844800 \t\t Average Reward : 0.49\n",
            "Episode : 2115 \t\t Timestep : 846400 \t\t Average Reward : 0.26\n",
            "Episode : 2119 \t\t Timestep : 848000 \t\t Average Reward : 0.44\n",
            "Episode : 2123 \t\t Timestep : 849600 \t\t Average Reward : 0.43\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.2\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2127 \t\t Timestep : 851200 \t\t Average Reward : 0.33\n",
            "Episode : 2131 \t\t Timestep : 852800 \t\t Average Reward : 0.4\n",
            "Episode : 2135 \t\t Timestep : 854400 \t\t Average Reward : 0.52\n",
            "Episode : 2139 \t\t Timestep : 856000 \t\t Average Reward : 0.36\n",
            "Episode : 2143 \t\t Timestep : 857600 \t\t Average Reward : 0.35\n",
            "Episode : 2147 \t\t Timestep : 859200 \t\t Average Reward : 0.32\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.18\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:23:44\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2151 \t\t Timestep : 860800 \t\t Average Reward : 0.43\n",
            "Episode : 2155 \t\t Timestep : 862400 \t\t Average Reward : 0.43\n",
            "Episode : 2159 \t\t Timestep : 864000 \t\t Average Reward : 0.35\n",
            "Episode : 2163 \t\t Timestep : 865600 \t\t Average Reward : 0.36\n",
            "Episode : 2167 \t\t Timestep : 867200 \t\t Average Reward : 0.39\n",
            "Episode : 2171 \t\t Timestep : 868800 \t\t Average Reward : 0.42\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.16\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2175 \t\t Timestep : 870400 \t\t Average Reward : 0.4\n",
            "Episode : 2179 \t\t Timestep : 872000 \t\t Average Reward : 0.44\n",
            "Episode : 2183 \t\t Timestep : 873600 \t\t Average Reward : 0.43\n",
            "Episode : 2187 \t\t Timestep : 875200 \t\t Average Reward : 0.46\n",
            "Episode : 2191 \t\t Timestep : 876800 \t\t Average Reward : 0.3\n",
            "Episode : 2195 \t\t Timestep : 878400 \t\t Average Reward : 0.4\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.14\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2199 \t\t Timestep : 880000 \t\t Average Reward : 0.26\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:24:16\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2203 \t\t Timestep : 881600 \t\t Average Reward : 0.47\n",
            "Episode : 2207 \t\t Timestep : 883200 \t\t Average Reward : 0.45\n",
            "Episode : 2211 \t\t Timestep : 884800 \t\t Average Reward : 0.46\n",
            "Episode : 2215 \t\t Timestep : 886400 \t\t Average Reward : 0.36\n",
            "Episode : 2219 \t\t Timestep : 888000 \t\t Average Reward : 0.31\n",
            "Episode : 2223 \t\t Timestep : 889600 \t\t Average Reward : 0.46\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.12\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2227 \t\t Timestep : 891200 \t\t Average Reward : 0.43\n",
            "Episode : 2231 \t\t Timestep : 892800 \t\t Average Reward : 0.52\n",
            "Episode : 2235 \t\t Timestep : 894400 \t\t Average Reward : 0.46\n",
            "Episode : 2239 \t\t Timestep : 896000 \t\t Average Reward : 0.39\n",
            "Episode : 2243 \t\t Timestep : 897600 \t\t Average Reward : 0.46\n",
            "Episode : 2247 \t\t Timestep : 899200 \t\t Average Reward : 0.53\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.1\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:24:47\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2251 \t\t Timestep : 900800 \t\t Average Reward : 0.3\n",
            "Episode : 2255 \t\t Timestep : 902400 \t\t Average Reward : 0.41\n",
            "Episode : 2259 \t\t Timestep : 904000 \t\t Average Reward : 0.33\n",
            "Episode : 2263 \t\t Timestep : 905600 \t\t Average Reward : 0.54\n",
            "Episode : 2267 \t\t Timestep : 907200 \t\t Average Reward : 0.43\n",
            "Episode : 2271 \t\t Timestep : 908800 \t\t Average Reward : 0.5\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.08\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2275 \t\t Timestep : 910400 \t\t Average Reward : 0.51\n",
            "Episode : 2279 \t\t Timestep : 912000 \t\t Average Reward : 0.44\n",
            "Episode : 2283 \t\t Timestep : 913600 \t\t Average Reward : 0.55\n",
            "Episode : 2287 \t\t Timestep : 915200 \t\t Average Reward : 0.55\n",
            "Episode : 2291 \t\t Timestep : 916800 \t\t Average Reward : 0.42\n",
            "Episode : 2295 \t\t Timestep : 918400 \t\t Average Reward : 0.34\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to :  0.06\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2299 \t\t Timestep : 920000 \t\t Average Reward : 0.54\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:25:19\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2303 \t\t Timestep : 921600 \t\t Average Reward : 0.54\n",
            "Episode : 2307 \t\t Timestep : 923200 \t\t Average Reward : 0.49\n",
            "Episode : 2311 \t\t Timestep : 924800 \t\t Average Reward : 0.55\n",
            "Episode : 2315 \t\t Timestep : 926400 \t\t Average Reward : 0.37\n",
            "Episode : 2319 \t\t Timestep : 928000 \t\t Average Reward : 0.42\n",
            "Episode : 2323 \t\t Timestep : 929600 \t\t Average Reward : 0.55\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2327 \t\t Timestep : 931200 \t\t Average Reward : 0.52\n",
            "Episode : 2331 \t\t Timestep : 932800 \t\t Average Reward : 0.53\n",
            "Episode : 2335 \t\t Timestep : 934400 \t\t Average Reward : 0.55\n",
            "Episode : 2339 \t\t Timestep : 936000 \t\t Average Reward : 0.55\n",
            "Episode : 2343 \t\t Timestep : 937600 \t\t Average Reward : 0.41\n",
            "Episode : 2347 \t\t Timestep : 939200 \t\t Average Reward : 0.46\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:25:51\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2351 \t\t Timestep : 940800 \t\t Average Reward : 0.57\n",
            "Episode : 2355 \t\t Timestep : 942400 \t\t Average Reward : 0.49\n",
            "Episode : 2359 \t\t Timestep : 944000 \t\t Average Reward : 0.55\n",
            "Episode : 2363 \t\t Timestep : 945600 \t\t Average Reward : 0.55\n",
            "Episode : 2367 \t\t Timestep : 947200 \t\t Average Reward : 0.32\n",
            "Episode : 2371 \t\t Timestep : 948800 \t\t Average Reward : 0.55\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2375 \t\t Timestep : 950400 \t\t Average Reward : 0.29\n",
            "Episode : 2379 \t\t Timestep : 952000 \t\t Average Reward : 0.28\n",
            "Episode : 2383 \t\t Timestep : 953600 \t\t Average Reward : 0.56\n",
            "Episode : 2387 \t\t Timestep : 955200 \t\t Average Reward : 0.44\n",
            "Episode : 2391 \t\t Timestep : 956800 \t\t Average Reward : 0.17\n",
            "Episode : 2395 \t\t Timestep : 958400 \t\t Average Reward : 0.24\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2399 \t\t Timestep : 960000 \t\t Average Reward : 0.39\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:26:23\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2403 \t\t Timestep : 961600 \t\t Average Reward : 0.36\n",
            "Episode : 2407 \t\t Timestep : 963200 \t\t Average Reward : 0.47\n",
            "Episode : 2411 \t\t Timestep : 964800 \t\t Average Reward : 0.46\n",
            "Episode : 2415 \t\t Timestep : 966400 \t\t Average Reward : 0.55\n",
            "Episode : 2419 \t\t Timestep : 968000 \t\t Average Reward : 0.43\n",
            "Episode : 2423 \t\t Timestep : 969600 \t\t Average Reward : 0.4\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2427 \t\t Timestep : 971200 \t\t Average Reward : 0.43\n",
            "Episode : 2431 \t\t Timestep : 972800 \t\t Average Reward : 0.41\n",
            "Episode : 2435 \t\t Timestep : 974400 \t\t Average Reward : 0.42\n",
            "Episode : 2439 \t\t Timestep : 976000 \t\t Average Reward : 0.26\n",
            "Episode : 2443 \t\t Timestep : 977600 \t\t Average Reward : 0.3\n",
            "Episode : 2447 \t\t Timestep : 979200 \t\t Average Reward : 0.42\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:26:55\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2451 \t\t Timestep : 980800 \t\t Average Reward : 0.55\n",
            "Episode : 2455 \t\t Timestep : 982400 \t\t Average Reward : 0.55\n",
            "Episode : 2459 \t\t Timestep : 984000 \t\t Average Reward : 0.59\n",
            "Episode : 2463 \t\t Timestep : 985600 \t\t Average Reward : 0.32\n",
            "Episode : 2467 \t\t Timestep : 987200 \t\t Average Reward : 0.6\n",
            "Episode : 2471 \t\t Timestep : 988800 \t\t Average Reward : 0.47\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2475 \t\t Timestep : 990400 \t\t Average Reward : 0.44\n",
            "Episode : 2479 \t\t Timestep : 992000 \t\t Average Reward : 0.42\n",
            "Episode : 2483 \t\t Timestep : 993600 \t\t Average Reward : 0.35\n",
            "Episode : 2487 \t\t Timestep : 995200 \t\t Average Reward : 0.47\n",
            "Episode : 2491 \t\t Timestep : 996800 \t\t Average Reward : 0.45\n",
            "Episode : 2495 \t\t Timestep : 998400 \t\t Average Reward : 0.54\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2499 \t\t Timestep : 1000000 \t\t Average Reward : 0.32\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:27:27\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2503 \t\t Timestep : 1001600 \t\t Average Reward : 0.55\n",
            "Episode : 2507 \t\t Timestep : 1003200 \t\t Average Reward : 0.35\n",
            "Episode : 2511 \t\t Timestep : 1004800 \t\t Average Reward : 0.55\n",
            "Episode : 2515 \t\t Timestep : 1006400 \t\t Average Reward : 0.43\n",
            "Episode : 2519 \t\t Timestep : 1008000 \t\t Average Reward : 0.55\n",
            "Episode : 2523 \t\t Timestep : 1009600 \t\t Average Reward : 0.46\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2527 \t\t Timestep : 1011200 \t\t Average Reward : 0.55\n",
            "Episode : 2531 \t\t Timestep : 1012800 \t\t Average Reward : 0.38\n",
            "Episode : 2535 \t\t Timestep : 1014400 \t\t Average Reward : 0.45\n",
            "Episode : 2539 \t\t Timestep : 1016000 \t\t Average Reward : 0.55\n",
            "Episode : 2543 \t\t Timestep : 1017600 \t\t Average Reward : 0.55\n",
            "Episode : 2547 \t\t Timestep : 1019200 \t\t Average Reward : 0.61\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:27:59\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2551 \t\t Timestep : 1020800 \t\t Average Reward : 0.48\n",
            "Episode : 2555 \t\t Timestep : 1022400 \t\t Average Reward : 0.4\n",
            "Episode : 2559 \t\t Timestep : 1024000 \t\t Average Reward : 0.38\n",
            "Episode : 2563 \t\t Timestep : 1025600 \t\t Average Reward : 0.43\n",
            "Episode : 2567 \t\t Timestep : 1027200 \t\t Average Reward : 0.35\n",
            "Episode : 2571 \t\t Timestep : 1028800 \t\t Average Reward : 0.32\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2575 \t\t Timestep : 1030400 \t\t Average Reward : 0.55\n",
            "Episode : 2579 \t\t Timestep : 1032000 \t\t Average Reward : 0.47\n",
            "Episode : 2583 \t\t Timestep : 1033600 \t\t Average Reward : 0.55\n",
            "Episode : 2587 \t\t Timestep : 1035200 \t\t Average Reward : 0.51\n",
            "Episode : 2591 \t\t Timestep : 1036800 \t\t Average Reward : 0.54\n",
            "Episode : 2595 \t\t Timestep : 1038400 \t\t Average Reward : 0.54\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2599 \t\t Timestep : 1040000 \t\t Average Reward : 0.55\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:28:31\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2603 \t\t Timestep : 1041600 \t\t Average Reward : 0.41\n",
            "Episode : 2607 \t\t Timestep : 1043200 \t\t Average Reward : 0.61\n",
            "Episode : 2611 \t\t Timestep : 1044800 \t\t Average Reward : 0.27\n",
            "Episode : 2615 \t\t Timestep : 1046400 \t\t Average Reward : 0.43\n",
            "Episode : 2619 \t\t Timestep : 1048000 \t\t Average Reward : 0.41\n",
            "Episode : 2623 \t\t Timestep : 1049600 \t\t Average Reward : 0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2627 \t\t Timestep : 1051200 \t\t Average Reward : 0.12\n",
            "Episode : 2631 \t\t Timestep : 1052800 \t\t Average Reward : 0.01\n",
            "Episode : 2635 \t\t Timestep : 1054400 \t\t Average Reward : 0.17\n",
            "Episode : 2639 \t\t Timestep : 1056000 \t\t Average Reward : 0.19\n",
            "Episode : 2643 \t\t Timestep : 1057600 \t\t Average Reward : -0.03\n",
            "Episode : 2647 \t\t Timestep : 1059200 \t\t Average Reward : 0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:29:02\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2651 \t\t Timestep : 1060800 \t\t Average Reward : 0.04\n",
            "Episode : 2655 \t\t Timestep : 1062400 \t\t Average Reward : 0.07\n",
            "Episode : 2659 \t\t Timestep : 1064000 \t\t Average Reward : 0.23\n",
            "Episode : 2663 \t\t Timestep : 1065600 \t\t Average Reward : 0.09\n",
            "Episode : 2667 \t\t Timestep : 1067200 \t\t Average Reward : 0.09\n",
            "Episode : 2671 \t\t Timestep : 1068800 \t\t Average Reward : 0.23\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2675 \t\t Timestep : 1070400 \t\t Average Reward : 0.17\n",
            "Episode : 2679 \t\t Timestep : 1072000 \t\t Average Reward : 0.08\n",
            "Episode : 2683 \t\t Timestep : 1073600 \t\t Average Reward : 0.08\n",
            "Episode : 2687 \t\t Timestep : 1075200 \t\t Average Reward : 0.22\n",
            "Episode : 2691 \t\t Timestep : 1076800 \t\t Average Reward : 0.13\n",
            "Episode : 2695 \t\t Timestep : 1078400 \t\t Average Reward : 0.15\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2699 \t\t Timestep : 1080000 \t\t Average Reward : 0.33\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:29:35\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2703 \t\t Timestep : 1081600 \t\t Average Reward : 0.26\n",
            "Episode : 2707 \t\t Timestep : 1083200 \t\t Average Reward : 0.15\n",
            "Episode : 2711 \t\t Timestep : 1084800 \t\t Average Reward : 0.13\n",
            "Episode : 2715 \t\t Timestep : 1086400 \t\t Average Reward : 0.17\n",
            "Episode : 2719 \t\t Timestep : 1088000 \t\t Average Reward : 0.37\n",
            "Episode : 2723 \t\t Timestep : 1089600 \t\t Average Reward : 0.43\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2727 \t\t Timestep : 1091200 \t\t Average Reward : 0.24\n",
            "Episode : 2731 \t\t Timestep : 1092800 \t\t Average Reward : 0.42\n",
            "Episode : 2735 \t\t Timestep : 1094400 \t\t Average Reward : 0.31\n",
            "Episode : 2739 \t\t Timestep : 1096000 \t\t Average Reward : 0.41\n",
            "Episode : 2743 \t\t Timestep : 1097600 \t\t Average Reward : 0.32\n",
            "Episode : 2747 \t\t Timestep : 1099200 \t\t Average Reward : 0.46\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:30:06\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2751 \t\t Timestep : 1100800 \t\t Average Reward : 0.16\n",
            "Episode : 2755 \t\t Timestep : 1102400 \t\t Average Reward : 0.2\n",
            "Episode : 2759 \t\t Timestep : 1104000 \t\t Average Reward : 0.21\n",
            "Episode : 2763 \t\t Timestep : 1105600 \t\t Average Reward : 0.25\n",
            "Episode : 2767 \t\t Timestep : 1107200 \t\t Average Reward : 0.34\n",
            "Episode : 2771 \t\t Timestep : 1108800 \t\t Average Reward : 0.25\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2775 \t\t Timestep : 1110400 \t\t Average Reward : 0.27\n",
            "Episode : 2779 \t\t Timestep : 1112000 \t\t Average Reward : 0.08\n",
            "Episode : 2783 \t\t Timestep : 1113600 \t\t Average Reward : 0.09\n",
            "Episode : 2787 \t\t Timestep : 1115200 \t\t Average Reward : 0.16\n",
            "Episode : 2791 \t\t Timestep : 1116800 \t\t Average Reward : -0.01\n",
            "Episode : 2795 \t\t Timestep : 1118400 \t\t Average Reward : 0.16\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2799 \t\t Timestep : 1120000 \t\t Average Reward : 0.22\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:30:38\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2803 \t\t Timestep : 1121600 \t\t Average Reward : 0.06\n",
            "Episode : 2807 \t\t Timestep : 1123200 \t\t Average Reward : 0.09\n",
            "Episode : 2811 \t\t Timestep : 1124800 \t\t Average Reward : 0.17\n",
            "Episode : 2815 \t\t Timestep : 1126400 \t\t Average Reward : -0.0\n",
            "Episode : 2819 \t\t Timestep : 1128000 \t\t Average Reward : 0.06\n",
            "Episode : 2823 \t\t Timestep : 1129600 \t\t Average Reward : 0.06\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2827 \t\t Timestep : 1131200 \t\t Average Reward : 0.03\n",
            "Episode : 2831 \t\t Timestep : 1132800 \t\t Average Reward : 0.17\n",
            "Episode : 2835 \t\t Timestep : 1134400 \t\t Average Reward : 0.28\n",
            "Episode : 2839 \t\t Timestep : 1136000 \t\t Average Reward : 0.4\n",
            "Episode : 2843 \t\t Timestep : 1137600 \t\t Average Reward : 0.17\n",
            "Episode : 2847 \t\t Timestep : 1139200 \t\t Average Reward : 0.15\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:31:10\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2851 \t\t Timestep : 1140800 \t\t Average Reward : 0.21\n",
            "Episode : 2855 \t\t Timestep : 1142400 \t\t Average Reward : 0.24\n",
            "Episode : 2859 \t\t Timestep : 1144000 \t\t Average Reward : 0.32\n",
            "Episode : 2863 \t\t Timestep : 1145600 \t\t Average Reward : 0.33\n",
            "Episode : 2867 \t\t Timestep : 1147200 \t\t Average Reward : 0.2\n",
            "Episode : 2871 \t\t Timestep : 1148800 \t\t Average Reward : 0.46\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2875 \t\t Timestep : 1150400 \t\t Average Reward : 0.17\n",
            "Episode : 2879 \t\t Timestep : 1152000 \t\t Average Reward : 0.29\n",
            "Episode : 2883 \t\t Timestep : 1153600 \t\t Average Reward : 0.18\n",
            "Episode : 2887 \t\t Timestep : 1155200 \t\t Average Reward : 0.2\n",
            "Episode : 2891 \t\t Timestep : 1156800 \t\t Average Reward : 0.35\n",
            "Episode : 2895 \t\t Timestep : 1158400 \t\t Average Reward : 0.35\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2899 \t\t Timestep : 1160000 \t\t Average Reward : 0.41\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:31:44\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2903 \t\t Timestep : 1161600 \t\t Average Reward : 0.22\n",
            "Episode : 2907 \t\t Timestep : 1163200 \t\t Average Reward : 0.45\n",
            "Episode : 2911 \t\t Timestep : 1164800 \t\t Average Reward : 0.58\n",
            "Episode : 2915 \t\t Timestep : 1166400 \t\t Average Reward : 0.57\n",
            "Episode : 2919 \t\t Timestep : 1168000 \t\t Average Reward : 0.55\n",
            "Episode : 2923 \t\t Timestep : 1169600 \t\t Average Reward : 0.55\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2927 \t\t Timestep : 1171200 \t\t Average Reward : 0.43\n",
            "Episode : 2931 \t\t Timestep : 1172800 \t\t Average Reward : 0.55\n",
            "Episode : 2935 \t\t Timestep : 1174400 \t\t Average Reward : 0.43\n",
            "Episode : 2939 \t\t Timestep : 1176000 \t\t Average Reward : 0.5\n",
            "Episode : 2943 \t\t Timestep : 1177600 \t\t Average Reward : 0.49\n",
            "Episode : 2947 \t\t Timestep : 1179200 \t\t Average Reward : 0.56\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:32:15\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2951 \t\t Timestep : 1180800 \t\t Average Reward : 0.55\n",
            "Episode : 2955 \t\t Timestep : 1182400 \t\t Average Reward : 0.55\n",
            "Episode : 2959 \t\t Timestep : 1184000 \t\t Average Reward : 0.3\n",
            "Episode : 2963 \t\t Timestep : 1185600 \t\t Average Reward : 0.47\n",
            "Episode : 2967 \t\t Timestep : 1187200 \t\t Average Reward : 0.5\n",
            "Episode : 2971 \t\t Timestep : 1188800 \t\t Average Reward : 0.34\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2975 \t\t Timestep : 1190400 \t\t Average Reward : 0.44\n",
            "Episode : 2979 \t\t Timestep : 1192000 \t\t Average Reward : 0.32\n",
            "Episode : 2983 \t\t Timestep : 1193600 \t\t Average Reward : 0.48\n",
            "Episode : 2987 \t\t Timestep : 1195200 \t\t Average Reward : 0.48\n",
            "Episode : 2991 \t\t Timestep : 1196800 \t\t Average Reward : 0.35\n",
            "Episode : 2995 \t\t Timestep : 1198400 \t\t Average Reward : 0.17\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 2999 \t\t Timestep : 1200000 \t\t Average Reward : 0.23\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:32:48\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3003 \t\t Timestep : 1201600 \t\t Average Reward : 0.51\n",
            "Episode : 3007 \t\t Timestep : 1203200 \t\t Average Reward : 0.5\n",
            "Episode : 3011 \t\t Timestep : 1204800 \t\t Average Reward : 0.13\n",
            "Episode : 3015 \t\t Timestep : 1206400 \t\t Average Reward : 0.07\n",
            "Episode : 3019 \t\t Timestep : 1208000 \t\t Average Reward : 0.27\n",
            "Episode : 3023 \t\t Timestep : 1209600 \t\t Average Reward : 0.25\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3027 \t\t Timestep : 1211200 \t\t Average Reward : 0.07\n",
            "Episode : 3031 \t\t Timestep : 1212800 \t\t Average Reward : 0.21\n",
            "Episode : 3035 \t\t Timestep : 1214400 \t\t Average Reward : 0.24\n",
            "Episode : 3039 \t\t Timestep : 1216000 \t\t Average Reward : 0.17\n",
            "Episode : 3043 \t\t Timestep : 1217600 \t\t Average Reward : 0.14\n",
            "Episode : 3047 \t\t Timestep : 1219200 \t\t Average Reward : 0.22\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:33:19\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3051 \t\t Timestep : 1220800 \t\t Average Reward : 0.12\n",
            "Episode : 3055 \t\t Timestep : 1222400 \t\t Average Reward : 0.02\n",
            "Episode : 3059 \t\t Timestep : 1224000 \t\t Average Reward : 0.36\n",
            "Episode : 3063 \t\t Timestep : 1225600 \t\t Average Reward : 0.12\n",
            "Episode : 3067 \t\t Timestep : 1227200 \t\t Average Reward : 0.3\n",
            "Episode : 3071 \t\t Timestep : 1228800 \t\t Average Reward : 0.22\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3075 \t\t Timestep : 1230400 \t\t Average Reward : 0.18\n",
            "Episode : 3079 \t\t Timestep : 1232000 \t\t Average Reward : 0.43\n",
            "Episode : 3083 \t\t Timestep : 1233600 \t\t Average Reward : 0.34\n",
            "Episode : 3087 \t\t Timestep : 1235200 \t\t Average Reward : 0.1\n",
            "Episode : 3091 \t\t Timestep : 1236800 \t\t Average Reward : 0.24\n",
            "Episode : 3095 \t\t Timestep : 1238400 \t\t Average Reward : 0.43\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3099 \t\t Timestep : 1240000 \t\t Average Reward : 0.26\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:33:52\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3103 \t\t Timestep : 1241600 \t\t Average Reward : 0.44\n",
            "Episode : 3107 \t\t Timestep : 1243200 \t\t Average Reward : 0.5\n",
            "Episode : 3111 \t\t Timestep : 1244800 \t\t Average Reward : 0.42\n",
            "Episode : 3115 \t\t Timestep : 1246400 \t\t Average Reward : 0.45\n",
            "Episode : 3119 \t\t Timestep : 1248000 \t\t Average Reward : 0.35\n",
            "Episode : 3123 \t\t Timestep : 1249600 \t\t Average Reward : 0.53\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3127 \t\t Timestep : 1251200 \t\t Average Reward : 0.48\n",
            "Episode : 3131 \t\t Timestep : 1252800 \t\t Average Reward : 0.43\n",
            "Episode : 3135 \t\t Timestep : 1254400 \t\t Average Reward : 0.55\n",
            "Episode : 3139 \t\t Timestep : 1256000 \t\t Average Reward : 0.49\n",
            "Episode : 3143 \t\t Timestep : 1257600 \t\t Average Reward : 0.43\n",
            "Episode : 3147 \t\t Timestep : 1259200 \t\t Average Reward : 0.54\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:34:23\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3151 \t\t Timestep : 1260800 \t\t Average Reward : 0.48\n",
            "Episode : 3155 \t\t Timestep : 1262400 \t\t Average Reward : 0.55\n",
            "Episode : 3159 \t\t Timestep : 1264000 \t\t Average Reward : 0.56\n",
            "Episode : 3163 \t\t Timestep : 1265600 \t\t Average Reward : 0.49\n",
            "Episode : 3167 \t\t Timestep : 1267200 \t\t Average Reward : 0.42\n",
            "Episode : 3171 \t\t Timestep : 1268800 \t\t Average Reward : 0.45\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3175 \t\t Timestep : 1270400 \t\t Average Reward : 0.55\n",
            "Episode : 3179 \t\t Timestep : 1272000 \t\t Average Reward : 0.32\n",
            "Episode : 3183 \t\t Timestep : 1273600 \t\t Average Reward : 0.21\n",
            "Episode : 3187 \t\t Timestep : 1275200 \t\t Average Reward : 0.33\n",
            "Episode : 3191 \t\t Timestep : 1276800 \t\t Average Reward : 0.36\n",
            "Episode : 3195 \t\t Timestep : 1278400 \t\t Average Reward : 0.33\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3199 \t\t Timestep : 1280000 \t\t Average Reward : 0.34\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:34:55\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3203 \t\t Timestep : 1281600 \t\t Average Reward : 0.35\n",
            "Episode : 3207 \t\t Timestep : 1283200 \t\t Average Reward : 0.55\n",
            "Episode : 3211 \t\t Timestep : 1284800 \t\t Average Reward : 0.32\n",
            "Episode : 3215 \t\t Timestep : 1286400 \t\t Average Reward : 0.44\n",
            "Episode : 3219 \t\t Timestep : 1288000 \t\t Average Reward : 0.32\n",
            "Episode : 3223 \t\t Timestep : 1289600 \t\t Average Reward : 0.42\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3227 \t\t Timestep : 1291200 \t\t Average Reward : 0.37\n",
            "Episode : 3231 \t\t Timestep : 1292800 \t\t Average Reward : 0.44\n",
            "Episode : 3235 \t\t Timestep : 1294400 \t\t Average Reward : 0.25\n",
            "Episode : 3239 \t\t Timestep : 1296000 \t\t Average Reward : 0.31\n",
            "Episode : 3243 \t\t Timestep : 1297600 \t\t Average Reward : 0.55\n",
            "Episode : 3247 \t\t Timestep : 1299200 \t\t Average Reward : 0.28\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:35:27\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3251 \t\t Timestep : 1300800 \t\t Average Reward : 0.2\n",
            "Episode : 3255 \t\t Timestep : 1302400 \t\t Average Reward : 0.25\n",
            "Episode : 3259 \t\t Timestep : 1304000 \t\t Average Reward : 0.19\n",
            "Episode : 3263 \t\t Timestep : 1305600 \t\t Average Reward : 0.21\n",
            "Episode : 3267 \t\t Timestep : 1307200 \t\t Average Reward : 0.47\n",
            "Episode : 3271 \t\t Timestep : 1308800 \t\t Average Reward : 0.61\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3275 \t\t Timestep : 1310400 \t\t Average Reward : 0.3\n",
            "Episode : 3279 \t\t Timestep : 1312000 \t\t Average Reward : 0.44\n",
            "Episode : 3283 \t\t Timestep : 1313600 \t\t Average Reward : 0.47\n",
            "Episode : 3287 \t\t Timestep : 1315200 \t\t Average Reward : 0.53\n",
            "Episode : 3291 \t\t Timestep : 1316800 \t\t Average Reward : 0.49\n",
            "Episode : 3295 \t\t Timestep : 1318400 \t\t Average Reward : 0.41\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3299 \t\t Timestep : 1320000 \t\t Average Reward : 0.48\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:35:59\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3303 \t\t Timestep : 1321600 \t\t Average Reward : 0.55\n",
            "Episode : 3307 \t\t Timestep : 1323200 \t\t Average Reward : 0.39\n",
            "Episode : 3311 \t\t Timestep : 1324800 \t\t Average Reward : 0.46\n",
            "Episode : 3315 \t\t Timestep : 1326400 \t\t Average Reward : 0.59\n",
            "Episode : 3319 \t\t Timestep : 1328000 \t\t Average Reward : 0.49\n",
            "Episode : 3323 \t\t Timestep : 1329600 \t\t Average Reward : 0.42\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3327 \t\t Timestep : 1331200 \t\t Average Reward : 0.54\n",
            "Episode : 3331 \t\t Timestep : 1332800 \t\t Average Reward : 0.55\n",
            "Episode : 3335 \t\t Timestep : 1334400 \t\t Average Reward : 0.54\n",
            "Episode : 3339 \t\t Timestep : 1336000 \t\t Average Reward : 0.53\n",
            "Episode : 3343 \t\t Timestep : 1337600 \t\t Average Reward : 0.54\n",
            "Episode : 3347 \t\t Timestep : 1339200 \t\t Average Reward : 0.58\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:36:30\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3351 \t\t Timestep : 1340800 \t\t Average Reward : 0.48\n",
            "Episode : 3355 \t\t Timestep : 1342400 \t\t Average Reward : 0.53\n",
            "Episode : 3359 \t\t Timestep : 1344000 \t\t Average Reward : 0.47\n",
            "Episode : 3363 \t\t Timestep : 1345600 \t\t Average Reward : 0.39\n",
            "Episode : 3367 \t\t Timestep : 1347200 \t\t Average Reward : 0.55\n",
            "Episode : 3371 \t\t Timestep : 1348800 \t\t Average Reward : 0.31\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3375 \t\t Timestep : 1350400 \t\t Average Reward : 0.38\n",
            "Episode : 3379 \t\t Timestep : 1352000 \t\t Average Reward : 0.5\n",
            "Episode : 3383 \t\t Timestep : 1353600 \t\t Average Reward : 0.37\n",
            "Episode : 3387 \t\t Timestep : 1355200 \t\t Average Reward : 0.49\n",
            "Episode : 3391 \t\t Timestep : 1356800 \t\t Average Reward : 0.46\n",
            "Episode : 3395 \t\t Timestep : 1358400 \t\t Average Reward : 0.38\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3399 \t\t Timestep : 1360000 \t\t Average Reward : 0.55\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:37:04\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3403 \t\t Timestep : 1361600 \t\t Average Reward : 0.35\n",
            "Episode : 3407 \t\t Timestep : 1363200 \t\t Average Reward : 0.43\n",
            "Episode : 3411 \t\t Timestep : 1364800 \t\t Average Reward : 0.54\n",
            "Episode : 3415 \t\t Timestep : 1366400 \t\t Average Reward : 0.47\n",
            "Episode : 3419 \t\t Timestep : 1368000 \t\t Average Reward : 0.47\n",
            "Episode : 3423 \t\t Timestep : 1369600 \t\t Average Reward : 0.51\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3427 \t\t Timestep : 1371200 \t\t Average Reward : 0.55\n",
            "Episode : 3431 \t\t Timestep : 1372800 \t\t Average Reward : 0.54\n",
            "Episode : 3435 \t\t Timestep : 1374400 \t\t Average Reward : 0.47\n",
            "Episode : 3439 \t\t Timestep : 1376000 \t\t Average Reward : 0.47\n",
            "Episode : 3443 \t\t Timestep : 1377600 \t\t Average Reward : 0.55\n",
            "Episode : 3447 \t\t Timestep : 1379200 \t\t Average Reward : 0.52\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:37:36\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3451 \t\t Timestep : 1380800 \t\t Average Reward : 0.39\n",
            "Episode : 3455 \t\t Timestep : 1382400 \t\t Average Reward : 0.37\n",
            "Episode : 3459 \t\t Timestep : 1384000 \t\t Average Reward : 0.47\n",
            "Episode : 3463 \t\t Timestep : 1385600 \t\t Average Reward : 0.48\n",
            "Episode : 3467 \t\t Timestep : 1387200 \t\t Average Reward : 0.43\n",
            "Episode : 3471 \t\t Timestep : 1388800 \t\t Average Reward : 0.45\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3475 \t\t Timestep : 1390400 \t\t Average Reward : 0.35\n",
            "Episode : 3479 \t\t Timestep : 1392000 \t\t Average Reward : 0.42\n",
            "Episode : 3483 \t\t Timestep : 1393600 \t\t Average Reward : 0.34\n",
            "Episode : 3487 \t\t Timestep : 1395200 \t\t Average Reward : 0.25\n",
            "Episode : 3491 \t\t Timestep : 1396800 \t\t Average Reward : 0.32\n",
            "Episode : 3495 \t\t Timestep : 1398400 \t\t Average Reward : 0.52\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3499 \t\t Timestep : 1400000 \t\t Average Reward : 0.29\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:38:09\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3503 \t\t Timestep : 1401600 \t\t Average Reward : 0.24\n",
            "Episode : 3507 \t\t Timestep : 1403200 \t\t Average Reward : 0.4\n",
            "Episode : 3511 \t\t Timestep : 1404800 \t\t Average Reward : 0.33\n",
            "Episode : 3515 \t\t Timestep : 1406400 \t\t Average Reward : 0.31\n",
            "Episode : 3519 \t\t Timestep : 1408000 \t\t Average Reward : 0.33\n",
            "Episode : 3523 \t\t Timestep : 1409600 \t\t Average Reward : 0.5\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3527 \t\t Timestep : 1411200 \t\t Average Reward : 0.4\n",
            "Episode : 3531 \t\t Timestep : 1412800 \t\t Average Reward : 0.44\n",
            "Episode : 3535 \t\t Timestep : 1414400 \t\t Average Reward : 0.55\n",
            "Episode : 3539 \t\t Timestep : 1416000 \t\t Average Reward : 0.55\n",
            "Episode : 3543 \t\t Timestep : 1417600 \t\t Average Reward : 0.23\n",
            "Episode : 3547 \t\t Timestep : 1419200 \t\t Average Reward : 0.4\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:38:41\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3551 \t\t Timestep : 1420800 \t\t Average Reward : 0.37\n",
            "Episode : 3555 \t\t Timestep : 1422400 \t\t Average Reward : 0.42\n",
            "Episode : 3559 \t\t Timestep : 1424000 \t\t Average Reward : 0.4\n",
            "Episode : 3563 \t\t Timestep : 1425600 \t\t Average Reward : 0.45\n",
            "Episode : 3567 \t\t Timestep : 1427200 \t\t Average Reward : 0.19\n",
            "Episode : 3571 \t\t Timestep : 1428800 \t\t Average Reward : 0.55\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3575 \t\t Timestep : 1430400 \t\t Average Reward : 0.45\n",
            "Episode : 3579 \t\t Timestep : 1432000 \t\t Average Reward : 0.41\n",
            "Episode : 3583 \t\t Timestep : 1433600 \t\t Average Reward : 0.43\n",
            "Episode : 3587 \t\t Timestep : 1435200 \t\t Average Reward : 0.55\n",
            "Episode : 3591 \t\t Timestep : 1436800 \t\t Average Reward : 0.54\n",
            "Episode : 3595 \t\t Timestep : 1438400 \t\t Average Reward : 0.32\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3599 \t\t Timestep : 1440000 \t\t Average Reward : 0.46\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:39:14\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3603 \t\t Timestep : 1441600 \t\t Average Reward : 0.27\n",
            "Episode : 3607 \t\t Timestep : 1443200 \t\t Average Reward : 0.18\n",
            "Episode : 3611 \t\t Timestep : 1444800 \t\t Average Reward : 0.32\n",
            "Episode : 3615 \t\t Timestep : 1446400 \t\t Average Reward : 0.03\n",
            "Episode : 3619 \t\t Timestep : 1448000 \t\t Average Reward : 0.25\n",
            "Episode : 3623 \t\t Timestep : 1449600 \t\t Average Reward : 0.03\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3627 \t\t Timestep : 1451200 \t\t Average Reward : 0.34\n",
            "Episode : 3631 \t\t Timestep : 1452800 \t\t Average Reward : 0.28\n",
            "Episode : 3635 \t\t Timestep : 1454400 \t\t Average Reward : 0.23\n",
            "Episode : 3639 \t\t Timestep : 1456000 \t\t Average Reward : 0.55\n",
            "Episode : 3643 \t\t Timestep : 1457600 \t\t Average Reward : 0.28\n",
            "Episode : 3647 \t\t Timestep : 1459200 \t\t Average Reward : 0.32\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:39:46\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3651 \t\t Timestep : 1460800 \t\t Average Reward : 0.25\n",
            "Episode : 3655 \t\t Timestep : 1462400 \t\t Average Reward : 0.22\n",
            "Episode : 3659 \t\t Timestep : 1464000 \t\t Average Reward : 0.33\n",
            "Episode : 3663 \t\t Timestep : 1465600 \t\t Average Reward : 0.16\n",
            "Episode : 3667 \t\t Timestep : 1467200 \t\t Average Reward : 0.1\n",
            "Episode : 3671 \t\t Timestep : 1468800 \t\t Average Reward : 0.23\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3675 \t\t Timestep : 1470400 \t\t Average Reward : 0.29\n",
            "Episode : 3679 \t\t Timestep : 1472000 \t\t Average Reward : 0.13\n",
            "Episode : 3683 \t\t Timestep : 1473600 \t\t Average Reward : 0.04\n",
            "Episode : 3687 \t\t Timestep : 1475200 \t\t Average Reward : 0.46\n",
            "Episode : 3691 \t\t Timestep : 1476800 \t\t Average Reward : 0.26\n",
            "Episode : 3695 \t\t Timestep : 1478400 \t\t Average Reward : 0.36\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3699 \t\t Timestep : 1480000 \t\t Average Reward : 0.43\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:40:19\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3703 \t\t Timestep : 1481600 \t\t Average Reward : 0.2\n",
            "Episode : 3707 \t\t Timestep : 1483200 \t\t Average Reward : 0.19\n",
            "Episode : 3711 \t\t Timestep : 1484800 \t\t Average Reward : 0.48\n",
            "Episode : 3715 \t\t Timestep : 1486400 \t\t Average Reward : 0.2\n",
            "Episode : 3719 \t\t Timestep : 1488000 \t\t Average Reward : 0.32\n",
            "Episode : 3723 \t\t Timestep : 1489600 \t\t Average Reward : 0.32\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3727 \t\t Timestep : 1491200 \t\t Average Reward : 0.02\n",
            "Episode : 3731 \t\t Timestep : 1492800 \t\t Average Reward : 0.15\n",
            "Episode : 3735 \t\t Timestep : 1494400 \t\t Average Reward : 0.17\n",
            "Episode : 3739 \t\t Timestep : 1496000 \t\t Average Reward : 0.29\n",
            "Episode : 3743 \t\t Timestep : 1497600 \t\t Average Reward : 0.12\n",
            "Episode : 3747 \t\t Timestep : 1499200 \t\t Average Reward : 0.1\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:40:51\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3751 \t\t Timestep : 1500800 \t\t Average Reward : 0.03\n",
            "Episode : 3755 \t\t Timestep : 1502400 \t\t Average Reward : 0.06\n",
            "Episode : 3759 \t\t Timestep : 1504000 \t\t Average Reward : 0.09\n",
            "Episode : 3763 \t\t Timestep : 1505600 \t\t Average Reward : 0.11\n",
            "Episode : 3767 \t\t Timestep : 1507200 \t\t Average Reward : 0.06\n",
            "Episode : 3771 \t\t Timestep : 1508800 \t\t Average Reward : 0.17\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3775 \t\t Timestep : 1510400 \t\t Average Reward : 0.19\n",
            "Episode : 3779 \t\t Timestep : 1512000 \t\t Average Reward : 0.25\n",
            "Episode : 3783 \t\t Timestep : 1513600 \t\t Average Reward : 0.21\n",
            "Episode : 3787 \t\t Timestep : 1515200 \t\t Average Reward : 0.28\n",
            "Episode : 3791 \t\t Timestep : 1516800 \t\t Average Reward : 0.13\n",
            "Episode : 3795 \t\t Timestep : 1518400 \t\t Average Reward : 0.3\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3799 \t\t Timestep : 1520000 \t\t Average Reward : 0.11\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:41:24\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3803 \t\t Timestep : 1521600 \t\t Average Reward : -0.04\n",
            "Episode : 3807 \t\t Timestep : 1523200 \t\t Average Reward : 0.06\n",
            "Episode : 3811 \t\t Timestep : 1524800 \t\t Average Reward : 0.14\n",
            "Episode : 3815 \t\t Timestep : 1526400 \t\t Average Reward : 0.06\n",
            "Episode : 3819 \t\t Timestep : 1528000 \t\t Average Reward : 0.17\n",
            "Episode : 3823 \t\t Timestep : 1529600 \t\t Average Reward : 0.1\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3827 \t\t Timestep : 1531200 \t\t Average Reward : 0.14\n",
            "Episode : 3831 \t\t Timestep : 1532800 \t\t Average Reward : 0.13\n",
            "Episode : 3835 \t\t Timestep : 1534400 \t\t Average Reward : 0.02\n",
            "Episode : 3839 \t\t Timestep : 1536000 \t\t Average Reward : -0.01\n",
            "Episode : 3843 \t\t Timestep : 1537600 \t\t Average Reward : 0.2\n",
            "Episode : 3847 \t\t Timestep : 1539200 \t\t Average Reward : 0.2\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:41:58\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3851 \t\t Timestep : 1540800 \t\t Average Reward : 0.08\n",
            "Episode : 3855 \t\t Timestep : 1542400 \t\t Average Reward : 0.14\n",
            "Episode : 3859 \t\t Timestep : 1544000 \t\t Average Reward : 0.14\n",
            "Episode : 3863 \t\t Timestep : 1545600 \t\t Average Reward : -0.01\n",
            "Episode : 3867 \t\t Timestep : 1547200 \t\t Average Reward : 0.25\n",
            "Episode : 3871 \t\t Timestep : 1548800 \t\t Average Reward : 0.07\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3875 \t\t Timestep : 1550400 \t\t Average Reward : 0.27\n",
            "Episode : 3879 \t\t Timestep : 1552000 \t\t Average Reward : 0.16\n",
            "Episode : 3883 \t\t Timestep : 1553600 \t\t Average Reward : 0.24\n",
            "Episode : 3887 \t\t Timestep : 1555200 \t\t Average Reward : 0.12\n",
            "Episode : 3891 \t\t Timestep : 1556800 \t\t Average Reward : 0.12\n",
            "Episode : 3895 \t\t Timestep : 1558400 \t\t Average Reward : 0.19\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3899 \t\t Timestep : 1560000 \t\t Average Reward : 0.09\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:42:30\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3903 \t\t Timestep : 1561600 \t\t Average Reward : 0.1\n",
            "Episode : 3907 \t\t Timestep : 1563200 \t\t Average Reward : 0.22\n",
            "Episode : 3911 \t\t Timestep : 1564800 \t\t Average Reward : 0.06\n",
            "Episode : 3915 \t\t Timestep : 1566400 \t\t Average Reward : 0.1\n",
            "Episode : 3919 \t\t Timestep : 1568000 \t\t Average Reward : 0.22\n",
            "Episode : 3923 \t\t Timestep : 1569600 \t\t Average Reward : 0.21\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3927 \t\t Timestep : 1571200 \t\t Average Reward : 0.07\n",
            "Episode : 3931 \t\t Timestep : 1572800 \t\t Average Reward : 0.06\n",
            "Episode : 3935 \t\t Timestep : 1574400 \t\t Average Reward : -0.03\n",
            "Episode : 3939 \t\t Timestep : 1576000 \t\t Average Reward : 0.02\n",
            "Episode : 3943 \t\t Timestep : 1577600 \t\t Average Reward : 0.14\n",
            "Episode : 3947 \t\t Timestep : 1579200 \t\t Average Reward : 0.06\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:43:02\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3951 \t\t Timestep : 1580800 \t\t Average Reward : 0.12\n",
            "Episode : 3955 \t\t Timestep : 1582400 \t\t Average Reward : 0.21\n",
            "Episode : 3959 \t\t Timestep : 1584000 \t\t Average Reward : 0.06\n",
            "Episode : 3963 \t\t Timestep : 1585600 \t\t Average Reward : 0.02\n",
            "Episode : 3967 \t\t Timestep : 1587200 \t\t Average Reward : 0.14\n",
            "Episode : 3971 \t\t Timestep : 1588800 \t\t Average Reward : 0.15\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3975 \t\t Timestep : 1590400 \t\t Average Reward : 0.08\n",
            "Episode : 3979 \t\t Timestep : 1592000 \t\t Average Reward : 0.11\n",
            "Episode : 3983 \t\t Timestep : 1593600 \t\t Average Reward : 0.07\n",
            "Episode : 3987 \t\t Timestep : 1595200 \t\t Average Reward : 0.22\n",
            "Episode : 3991 \t\t Timestep : 1596800 \t\t Average Reward : 0.18\n",
            "Episode : 3995 \t\t Timestep : 1598400 \t\t Average Reward : 0.18\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 3999 \t\t Timestep : 1600000 \t\t Average Reward : 0.24\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:43:35\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4003 \t\t Timestep : 1601600 \t\t Average Reward : 0.32\n",
            "Episode : 4007 \t\t Timestep : 1603200 \t\t Average Reward : 0.23\n",
            "Episode : 4011 \t\t Timestep : 1604800 \t\t Average Reward : 0.03\n",
            "Episode : 4015 \t\t Timestep : 1606400 \t\t Average Reward : 0.08\n",
            "Episode : 4019 \t\t Timestep : 1608000 \t\t Average Reward : 0.3\n",
            "Episode : 4023 \t\t Timestep : 1609600 \t\t Average Reward : 0.13\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4027 \t\t Timestep : 1611200 \t\t Average Reward : 0.28\n",
            "Episode : 4031 \t\t Timestep : 1612800 \t\t Average Reward : 0.04\n",
            "Episode : 4035 \t\t Timestep : 1614400 \t\t Average Reward : 0.14\n",
            "Episode : 4039 \t\t Timestep : 1616000 \t\t Average Reward : 0.2\n",
            "Episode : 4043 \t\t Timestep : 1617600 \t\t Average Reward : 0.24\n",
            "Episode : 4047 \t\t Timestep : 1619200 \t\t Average Reward : 0.29\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:44:07\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4051 \t\t Timestep : 1620800 \t\t Average Reward : 0.04\n",
            "Episode : 4055 \t\t Timestep : 1622400 \t\t Average Reward : 0.07\n",
            "Episode : 4059 \t\t Timestep : 1624000 \t\t Average Reward : 0.15\n",
            "Episode : 4063 \t\t Timestep : 1625600 \t\t Average Reward : 0.23\n",
            "Episode : 4067 \t\t Timestep : 1627200 \t\t Average Reward : 0.24\n",
            "Episode : 4071 \t\t Timestep : 1628800 \t\t Average Reward : 0.46\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4075 \t\t Timestep : 1630400 \t\t Average Reward : 0.5\n",
            "Episode : 4079 \t\t Timestep : 1632000 \t\t Average Reward : 0.49\n",
            "Episode : 4083 \t\t Timestep : 1633600 \t\t Average Reward : 0.52\n",
            "Episode : 4087 \t\t Timestep : 1635200 \t\t Average Reward : 0.6\n",
            "Episode : 4091 \t\t Timestep : 1636800 \t\t Average Reward : 0.57\n",
            "Episode : 4095 \t\t Timestep : 1638400 \t\t Average Reward : 0.55\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4099 \t\t Timestep : 1640000 \t\t Average Reward : 0.47\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:44:39\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4103 \t\t Timestep : 1641600 \t\t Average Reward : 0.55\n",
            "Episode : 4107 \t\t Timestep : 1643200 \t\t Average Reward : 0.42\n",
            "Episode : 4111 \t\t Timestep : 1644800 \t\t Average Reward : 0.55\n",
            "Episode : 4115 \t\t Timestep : 1646400 \t\t Average Reward : 0.38\n",
            "Episode : 4119 \t\t Timestep : 1648000 \t\t Average Reward : 0.54\n",
            "Episode : 4123 \t\t Timestep : 1649600 \t\t Average Reward : 0.44\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4127 \t\t Timestep : 1651200 \t\t Average Reward : 0.44\n",
            "Episode : 4131 \t\t Timestep : 1652800 \t\t Average Reward : 0.55\n",
            "Episode : 4135 \t\t Timestep : 1654400 \t\t Average Reward : 0.5\n",
            "Episode : 4139 \t\t Timestep : 1656000 \t\t Average Reward : 0.55\n",
            "Episode : 4143 \t\t Timestep : 1657600 \t\t Average Reward : 0.48\n",
            "Episode : 4147 \t\t Timestep : 1659200 \t\t Average Reward : 0.49\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:45:11\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4151 \t\t Timestep : 1660800 \t\t Average Reward : 0.53\n",
            "Episode : 4155 \t\t Timestep : 1662400 \t\t Average Reward : 0.52\n",
            "Episode : 4159 \t\t Timestep : 1664000 \t\t Average Reward : 0.42\n",
            "Episode : 4163 \t\t Timestep : 1665600 \t\t Average Reward : 0.49\n",
            "Episode : 4167 \t\t Timestep : 1667200 \t\t Average Reward : 0.42\n",
            "Episode : 4171 \t\t Timestep : 1668800 \t\t Average Reward : 0.49\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4175 \t\t Timestep : 1670400 \t\t Average Reward : 0.44\n",
            "Episode : 4179 \t\t Timestep : 1672000 \t\t Average Reward : 0.55\n",
            "Episode : 4183 \t\t Timestep : 1673600 \t\t Average Reward : 0.5\n",
            "Episode : 4187 \t\t Timestep : 1675200 \t\t Average Reward : 0.46\n",
            "Episode : 4191 \t\t Timestep : 1676800 \t\t Average Reward : 0.49\n",
            "Episode : 4195 \t\t Timestep : 1678400 \t\t Average Reward : 0.44\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4199 \t\t Timestep : 1680000 \t\t Average Reward : 0.39\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:45:43\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4203 \t\t Timestep : 1681600 \t\t Average Reward : 0.37\n",
            "Episode : 4207 \t\t Timestep : 1683200 \t\t Average Reward : 0.44\n",
            "Episode : 4211 \t\t Timestep : 1684800 \t\t Average Reward : 0.44\n",
            "Episode : 4215 \t\t Timestep : 1686400 \t\t Average Reward : 0.44\n",
            "Episode : 4219 \t\t Timestep : 1688000 \t\t Average Reward : 0.41\n",
            "Episode : 4223 \t\t Timestep : 1689600 \t\t Average Reward : 0.44\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4227 \t\t Timestep : 1691200 \t\t Average Reward : 0.5\n",
            "Episode : 4231 \t\t Timestep : 1692800 \t\t Average Reward : 0.5\n",
            "Episode : 4235 \t\t Timestep : 1694400 \t\t Average Reward : 0.44\n",
            "Episode : 4239 \t\t Timestep : 1696000 \t\t Average Reward : 0.5\n",
            "Episode : 4243 \t\t Timestep : 1697600 \t\t Average Reward : 0.37\n",
            "Episode : 4247 \t\t Timestep : 1699200 \t\t Average Reward : 0.31\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:46:16\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4251 \t\t Timestep : 1700800 \t\t Average Reward : 0.44\n",
            "Episode : 4255 \t\t Timestep : 1702400 \t\t Average Reward : 0.36\n",
            "Episode : 4259 \t\t Timestep : 1704000 \t\t Average Reward : 0.42\n",
            "Episode : 4263 \t\t Timestep : 1705600 \t\t Average Reward : 0.37\n",
            "Episode : 4267 \t\t Timestep : 1707200 \t\t Average Reward : 0.35\n",
            "Episode : 4271 \t\t Timestep : 1708800 \t\t Average Reward : 0.5\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4275 \t\t Timestep : 1710400 \t\t Average Reward : 0.53\n",
            "Episode : 4279 \t\t Timestep : 1712000 \t\t Average Reward : 0.5\n",
            "Episode : 4283 \t\t Timestep : 1713600 \t\t Average Reward : 0.41\n",
            "Episode : 4287 \t\t Timestep : 1715200 \t\t Average Reward : 0.44\n",
            "Episode : 4291 \t\t Timestep : 1716800 \t\t Average Reward : 0.55\n",
            "Episode : 4295 \t\t Timestep : 1718400 \t\t Average Reward : 0.55\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4299 \t\t Timestep : 1720000 \t\t Average Reward : 0.55\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:46:49\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4303 \t\t Timestep : 1721600 \t\t Average Reward : 0.44\n",
            "Episode : 4307 \t\t Timestep : 1723200 \t\t Average Reward : 0.55\n",
            "Episode : 4311 \t\t Timestep : 1724800 \t\t Average Reward : 0.55\n",
            "Episode : 4315 \t\t Timestep : 1726400 \t\t Average Reward : 0.55\n",
            "Episode : 4319 \t\t Timestep : 1728000 \t\t Average Reward : 0.55\n",
            "Episode : 4323 \t\t Timestep : 1729600 \t\t Average Reward : 0.55\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4327 \t\t Timestep : 1731200 \t\t Average Reward : 0.55\n",
            "Episode : 4331 \t\t Timestep : 1732800 \t\t Average Reward : 0.55\n",
            "Episode : 4335 \t\t Timestep : 1734400 \t\t Average Reward : 0.55\n",
            "Episode : 4339 \t\t Timestep : 1736000 \t\t Average Reward : 0.55\n",
            "Episode : 4343 \t\t Timestep : 1737600 \t\t Average Reward : 0.55\n",
            "Episode : 4347 \t\t Timestep : 1739200 \t\t Average Reward : 0.61\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:47:22\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4351 \t\t Timestep : 1740800 \t\t Average Reward : 0.55\n",
            "Episode : 4355 \t\t Timestep : 1742400 \t\t Average Reward : 0.55\n",
            "Episode : 4359 \t\t Timestep : 1744000 \t\t Average Reward : 0.46\n",
            "Episode : 4363 \t\t Timestep : 1745600 \t\t Average Reward : 0.55\n",
            "Episode : 4367 \t\t Timestep : 1747200 \t\t Average Reward : 0.52\n",
            "Episode : 4371 \t\t Timestep : 1748800 \t\t Average Reward : 0.55\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4375 \t\t Timestep : 1750400 \t\t Average Reward : 0.5\n",
            "Episode : 4379 \t\t Timestep : 1752000 \t\t Average Reward : 0.5\n",
            "Episode : 4383 \t\t Timestep : 1753600 \t\t Average Reward : 0.55\n",
            "Episode : 4387 \t\t Timestep : 1755200 \t\t Average Reward : 0.5\n",
            "Episode : 4391 \t\t Timestep : 1756800 \t\t Average Reward : 0.44\n",
            "Episode : 4395 \t\t Timestep : 1758400 \t\t Average Reward : 0.47\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4399 \t\t Timestep : 1760000 \t\t Average Reward : 0.54\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:47:54\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4403 \t\t Timestep : 1761600 \t\t Average Reward : 0.55\n",
            "Episode : 4407 \t\t Timestep : 1763200 \t\t Average Reward : 0.55\n",
            "Episode : 4411 \t\t Timestep : 1764800 \t\t Average Reward : 0.46\n",
            "Episode : 4415 \t\t Timestep : 1766400 \t\t Average Reward : 0.55\n",
            "Episode : 4419 \t\t Timestep : 1768000 \t\t Average Reward : 0.46\n",
            "Episode : 4423 \t\t Timestep : 1769600 \t\t Average Reward : 0.5\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4427 \t\t Timestep : 1771200 \t\t Average Reward : 0.55\n",
            "Episode : 4431 \t\t Timestep : 1772800 \t\t Average Reward : 0.55\n",
            "Episode : 4435 \t\t Timestep : 1774400 \t\t Average Reward : 0.5\n",
            "Episode : 4439 \t\t Timestep : 1776000 \t\t Average Reward : 0.55\n",
            "Episode : 4443 \t\t Timestep : 1777600 \t\t Average Reward : 0.28\n",
            "Episode : 4447 \t\t Timestep : 1779200 \t\t Average Reward : 0.22\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:48:26\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4451 \t\t Timestep : 1780800 \t\t Average Reward : 0.23\n",
            "Episode : 4455 \t\t Timestep : 1782400 \t\t Average Reward : 0.3\n",
            "Episode : 4459 \t\t Timestep : 1784000 \t\t Average Reward : 0.27\n",
            "Episode : 4463 \t\t Timestep : 1785600 \t\t Average Reward : 0.33\n",
            "Episode : 4467 \t\t Timestep : 1787200 \t\t Average Reward : 0.38\n",
            "Episode : 4471 \t\t Timestep : 1788800 \t\t Average Reward : 0.21\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4475 \t\t Timestep : 1790400 \t\t Average Reward : 0.17\n",
            "Episode : 4479 \t\t Timestep : 1792000 \t\t Average Reward : 0.22\n",
            "Episode : 4483 \t\t Timestep : 1793600 \t\t Average Reward : 0.28\n",
            "Episode : 4487 \t\t Timestep : 1795200 \t\t Average Reward : 0.32\n",
            "Episode : 4491 \t\t Timestep : 1796800 \t\t Average Reward : 0.4\n",
            "Episode : 4495 \t\t Timestep : 1798400 \t\t Average Reward : 0.36\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4499 \t\t Timestep : 1800000 \t\t Average Reward : 0.35\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:48:59\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4503 \t\t Timestep : 1801600 \t\t Average Reward : 0.23\n",
            "Episode : 4507 \t\t Timestep : 1803200 \t\t Average Reward : 0.26\n",
            "Episode : 4511 \t\t Timestep : 1804800 \t\t Average Reward : 0.3\n",
            "Episode : 4515 \t\t Timestep : 1806400 \t\t Average Reward : 0.55\n",
            "Episode : 4519 \t\t Timestep : 1808000 \t\t Average Reward : 0.49\n",
            "Episode : 4523 \t\t Timestep : 1809600 \t\t Average Reward : 0.39\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4527 \t\t Timestep : 1811200 \t\t Average Reward : 0.46\n",
            "Episode : 4531 \t\t Timestep : 1812800 \t\t Average Reward : 0.45\n",
            "Episode : 4535 \t\t Timestep : 1814400 \t\t Average Reward : 0.55\n",
            "Episode : 4539 \t\t Timestep : 1816000 \t\t Average Reward : 0.43\n",
            "Episode : 4543 \t\t Timestep : 1817600 \t\t Average Reward : 0.47\n",
            "Episode : 4547 \t\t Timestep : 1819200 \t\t Average Reward : 0.55\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:49:32\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4551 \t\t Timestep : 1820800 \t\t Average Reward : 0.47\n",
            "Episode : 4555 \t\t Timestep : 1822400 \t\t Average Reward : 0.49\n",
            "Episode : 4559 \t\t Timestep : 1824000 \t\t Average Reward : 0.41\n",
            "Episode : 4563 \t\t Timestep : 1825600 \t\t Average Reward : 0.51\n",
            "Episode : 4567 \t\t Timestep : 1827200 \t\t Average Reward : 0.48\n",
            "Episode : 4571 \t\t Timestep : 1828800 \t\t Average Reward : 0.35\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4575 \t\t Timestep : 1830400 \t\t Average Reward : 0.53\n",
            "Episode : 4579 \t\t Timestep : 1832000 \t\t Average Reward : 0.48\n",
            "Episode : 4583 \t\t Timestep : 1833600 \t\t Average Reward : 0.51\n",
            "Episode : 4587 \t\t Timestep : 1835200 \t\t Average Reward : 0.55\n",
            "Episode : 4591 \t\t Timestep : 1836800 \t\t Average Reward : 0.51\n",
            "Episode : 4595 \t\t Timestep : 1838400 \t\t Average Reward : 0.54\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4599 \t\t Timestep : 1840000 \t\t Average Reward : 0.45\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:50:08\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4603 \t\t Timestep : 1841600 \t\t Average Reward : 0.51\n",
            "Episode : 4607 \t\t Timestep : 1843200 \t\t Average Reward : 0.53\n",
            "Episode : 4611 \t\t Timestep : 1844800 \t\t Average Reward : 0.55\n",
            "Episode : 4615 \t\t Timestep : 1846400 \t\t Average Reward : 0.42\n",
            "Episode : 4619 \t\t Timestep : 1848000 \t\t Average Reward : 0.58\n",
            "Episode : 4623 \t\t Timestep : 1849600 \t\t Average Reward : 0.55\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4627 \t\t Timestep : 1851200 \t\t Average Reward : 0.56\n",
            "Episode : 4631 \t\t Timestep : 1852800 \t\t Average Reward : 0.55\n",
            "Episode : 4635 \t\t Timestep : 1854400 \t\t Average Reward : 0.45\n",
            "Episode : 4639 \t\t Timestep : 1856000 \t\t Average Reward : 0.55\n",
            "Episode : 4643 \t\t Timestep : 1857600 \t\t Average Reward : 0.36\n",
            "Episode : 4647 \t\t Timestep : 1859200 \t\t Average Reward : 0.19\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:50:42\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4651 \t\t Timestep : 1860800 \t\t Average Reward : 0.18\n",
            "Episode : 4655 \t\t Timestep : 1862400 \t\t Average Reward : 0.12\n",
            "Episode : 4659 \t\t Timestep : 1864000 \t\t Average Reward : 0.19\n",
            "Episode : 4663 \t\t Timestep : 1865600 \t\t Average Reward : 0.13\n",
            "Episode : 4667 \t\t Timestep : 1867200 \t\t Average Reward : 0.35\n",
            "Episode : 4671 \t\t Timestep : 1868800 \t\t Average Reward : 0.46\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4675 \t\t Timestep : 1870400 \t\t Average Reward : 0.42\n",
            "Episode : 4679 \t\t Timestep : 1872000 \t\t Average Reward : 0.28\n",
            "Episode : 4683 \t\t Timestep : 1873600 \t\t Average Reward : 0.37\n",
            "Episode : 4687 \t\t Timestep : 1875200 \t\t Average Reward : 0.29\n",
            "Episode : 4691 \t\t Timestep : 1876800 \t\t Average Reward : 0.09\n",
            "Episode : 4695 \t\t Timestep : 1878400 \t\t Average Reward : 0.23\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4699 \t\t Timestep : 1880000 \t\t Average Reward : 0.22\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:51:15\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4703 \t\t Timestep : 1881600 \t\t Average Reward : 0.02\n",
            "Episode : 4707 \t\t Timestep : 1883200 \t\t Average Reward : 0.19\n",
            "Episode : 4711 \t\t Timestep : 1884800 \t\t Average Reward : 0.07\n",
            "Episode : 4715 \t\t Timestep : 1886400 \t\t Average Reward : 0.24\n",
            "Episode : 4719 \t\t Timestep : 1888000 \t\t Average Reward : 0.11\n",
            "Episode : 4723 \t\t Timestep : 1889600 \t\t Average Reward : 0.1\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4727 \t\t Timestep : 1891200 \t\t Average Reward : 0.21\n",
            "Episode : 4731 \t\t Timestep : 1892800 \t\t Average Reward : 0.28\n",
            "Episode : 4735 \t\t Timestep : 1894400 \t\t Average Reward : 0.3\n",
            "Episode : 4739 \t\t Timestep : 1896000 \t\t Average Reward : 0.24\n",
            "Episode : 4743 \t\t Timestep : 1897600 \t\t Average Reward : 0.37\n",
            "Episode : 4747 \t\t Timestep : 1899200 \t\t Average Reward : 0.29\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:51:48\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4751 \t\t Timestep : 1900800 \t\t Average Reward : 0.33\n",
            "Episode : 4755 \t\t Timestep : 1902400 \t\t Average Reward : 0.1\n",
            "Episode : 4759 \t\t Timestep : 1904000 \t\t Average Reward : 0.55\n",
            "Episode : 4763 \t\t Timestep : 1905600 \t\t Average Reward : 0.32\n",
            "Episode : 4767 \t\t Timestep : 1907200 \t\t Average Reward : 0.49\n",
            "Episode : 4771 \t\t Timestep : 1908800 \t\t Average Reward : 0.35\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4775 \t\t Timestep : 1910400 \t\t Average Reward : 0.34\n",
            "Episode : 4779 \t\t Timestep : 1912000 \t\t Average Reward : 0.44\n",
            "Episode : 4783 \t\t Timestep : 1913600 \t\t Average Reward : 0.3\n",
            "Episode : 4787 \t\t Timestep : 1915200 \t\t Average Reward : 0.3\n",
            "Episode : 4791 \t\t Timestep : 1916800 \t\t Average Reward : 0.46\n",
            "Episode : 4795 \t\t Timestep : 1918400 \t\t Average Reward : 0.39\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4799 \t\t Timestep : 1920000 \t\t Average Reward : 0.55\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:52:22\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4803 \t\t Timestep : 1921600 \t\t Average Reward : 0.53\n",
            "Episode : 4807 \t\t Timestep : 1923200 \t\t Average Reward : 0.33\n",
            "Episode : 4811 \t\t Timestep : 1924800 \t\t Average Reward : 0.51\n",
            "Episode : 4815 \t\t Timestep : 1926400 \t\t Average Reward : 0.55\n",
            "Episode : 4819 \t\t Timestep : 1928000 \t\t Average Reward : 0.41\n",
            "Episode : 4823 \t\t Timestep : 1929600 \t\t Average Reward : 0.34\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4827 \t\t Timestep : 1931200 \t\t Average Reward : 0.51\n",
            "Episode : 4831 \t\t Timestep : 1932800 \t\t Average Reward : 0.53\n",
            "Episode : 4835 \t\t Timestep : 1934400 \t\t Average Reward : 0.55\n",
            "Episode : 4839 \t\t Timestep : 1936000 \t\t Average Reward : 0.41\n",
            "Episode : 4843 \t\t Timestep : 1937600 \t\t Average Reward : 0.52\n",
            "Episode : 4847 \t\t Timestep : 1939200 \t\t Average Reward : 0.51\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:52:54\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4851 \t\t Timestep : 1940800 \t\t Average Reward : 0.49\n",
            "Episode : 4855 \t\t Timestep : 1942400 \t\t Average Reward : 0.51\n",
            "Episode : 4859 \t\t Timestep : 1944000 \t\t Average Reward : 0.51\n",
            "Episode : 4863 \t\t Timestep : 1945600 \t\t Average Reward : 0.52\n",
            "Episode : 4867 \t\t Timestep : 1947200 \t\t Average Reward : 0.52\n",
            "Episode : 4871 \t\t Timestep : 1948800 \t\t Average Reward : 0.52\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4875 \t\t Timestep : 1950400 \t\t Average Reward : 0.55\n",
            "Episode : 4879 \t\t Timestep : 1952000 \t\t Average Reward : 0.5\n",
            "Episode : 4883 \t\t Timestep : 1953600 \t\t Average Reward : 0.53\n",
            "Episode : 4887 \t\t Timestep : 1955200 \t\t Average Reward : 0.41\n",
            "Episode : 4891 \t\t Timestep : 1956800 \t\t Average Reward : 0.54\n",
            "Episode : 4895 \t\t Timestep : 1958400 \t\t Average Reward : 0.53\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4899 \t\t Timestep : 1960000 \t\t Average Reward : 0.53\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:53:27\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4903 \t\t Timestep : 1961600 \t\t Average Reward : 0.52\n",
            "Episode : 4907 \t\t Timestep : 1963200 \t\t Average Reward : 0.52\n",
            "Episode : 4911 \t\t Timestep : 1964800 \t\t Average Reward : 0.43\n",
            "Episode : 4915 \t\t Timestep : 1966400 \t\t Average Reward : 0.47\n",
            "Episode : 4919 \t\t Timestep : 1968000 \t\t Average Reward : 0.43\n",
            "Episode : 4923 \t\t Timestep : 1969600 \t\t Average Reward : 0.43\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4927 \t\t Timestep : 1971200 \t\t Average Reward : 0.46\n",
            "Episode : 4931 \t\t Timestep : 1972800 \t\t Average Reward : 0.54\n",
            "Episode : 4935 \t\t Timestep : 1974400 \t\t Average Reward : 0.28\n",
            "Episode : 4939 \t\t Timestep : 1976000 \t\t Average Reward : 0.55\n",
            "Episode : 4943 \t\t Timestep : 1977600 \t\t Average Reward : 0.22\n",
            "Episode : 4947 \t\t Timestep : 1979200 \t\t Average Reward : 0.54\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:54:00\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4951 \t\t Timestep : 1980800 \t\t Average Reward : 0.45\n",
            "Episode : 4955 \t\t Timestep : 1982400 \t\t Average Reward : 0.38\n",
            "Episode : 4959 \t\t Timestep : 1984000 \t\t Average Reward : 0.48\n",
            "Episode : 4963 \t\t Timestep : 1985600 \t\t Average Reward : 0.48\n",
            "Episode : 4967 \t\t Timestep : 1987200 \t\t Average Reward : 0.35\n",
            "Episode : 4971 \t\t Timestep : 1988800 \t\t Average Reward : 0.2\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4975 \t\t Timestep : 1990400 \t\t Average Reward : 0.25\n",
            "Episode : 4979 \t\t Timestep : 1992000 \t\t Average Reward : 0.13\n",
            "Episode : 4983 \t\t Timestep : 1993600 \t\t Average Reward : 0.01\n",
            "Episode : 4987 \t\t Timestep : 1995200 \t\t Average Reward : 0.33\n",
            "Episode : 4991 \t\t Timestep : 1996800 \t\t Average Reward : 0.32\n",
            "Episode : 4995 \t\t Timestep : 1998400 \t\t Average Reward : 0.25\n",
            "--------------------------------------------------------------------------------------------\n",
            "setting actor output action_std to min_action_std :  0.05\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 4999 \t\t Timestep : 2000000 \t\t Average Reward : 0.31\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/Inverted Pendulum in MuJoCo/PPO_Inverted Pendulum in MuJoCo_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:54:35\n",
            "--------------------------------------------------------------------------------------------\n",
            "============================================================================================\n",
            "Started training at (GMT) :  2024-12-11 11:51:12\n",
            "Finished training at (GMT) :  2024-12-11 12:45:48\n",
            "Total training time  :  0:54:36\n",
            "============================================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "################################### Training ###################################\n",
        "\n",
        "\n",
        "####### initialize environment hyperparameters ######\n",
        "\n",
        "env_name = \"Inverted Pendulum in MuJoCo\"\n",
        "env = InvertedPendulumEnv()\n",
        "has_continuous_action_space = True\n",
        "action_std = 1.9\n",
        "action_std_decay_rate = 0.02\n",
        "min_action_std = 0.05\n",
        "action_std_decay_freq = 10000\n",
        "\n",
        "max_ep_len = 400                    # max timesteps in one episode\n",
        "max_training_timesteps = int(20e5)   # break training loop if timeteps > max_training_timesteps\n",
        "\n",
        "print_freq = max_ep_len * 4     # print avg reward in the interval (in num timesteps)\n",
        "log_freq = max_ep_len * 2       # log avg reward in the interval (in num timesteps)\n",
        "save_model_freq = int(2e4)      # save model frequency (in num timesteps)\n",
        "\n",
        "\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "## Note : print/log frequencies should be > than max_ep_len\n",
        "\n",
        "\n",
        "################ PPO hyperparameters ################\n",
        "\n",
        "\n",
        "update_timestep = max_ep_len * 4      # update policy every n timesteps\n",
        "K_epochs = 40               # update policy for K epochs\n",
        "eps_clip = 0.2              # clip parameter for PPO\n",
        "gamma = 0.98                # discount factor\n",
        "\n",
        "lr_actor = 0.0001       # learning rate for actor network\n",
        "lr_critic = 0.0003       # learning rate for critic network\n",
        "\n",
        "random_seed = 0         # set random seed if required (0 = no random seed)\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "\n",
        "print(\"training environment name : \" + env_name)\n",
        "\n",
        "#env = gym.make(env_name)\n",
        "\n",
        "# state space dimension\n",
        "state_dim = env.observation_space.shape[0]\n",
        "\n",
        "# action space dimension\n",
        "if has_continuous_action_space:\n",
        "    action_dim = env.action_space.shape[0]\n",
        "else:\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "\n",
        "\n",
        "###################### logging ######################\n",
        "\n",
        "#### log files for multiple runs are NOT overwritten\n",
        "\n",
        "log_dir = \"PPO_logs\"\n",
        "if not os.path.exists(log_dir):\n",
        "      os.makedirs(log_dir)\n",
        "\n",
        "log_dir = log_dir + '/' + env_name + '/'\n",
        "if not os.path.exists(log_dir):\n",
        "      os.makedirs(log_dir)\n",
        "\n",
        "\n",
        "#### get number of log files in log directory\n",
        "run_num = 0\n",
        "current_num_files = next(os.walk(log_dir))[2]\n",
        "run_num = len(current_num_files)\n",
        "\n",
        "\n",
        "#### create new log file for each run\n",
        "log_f_name = log_dir + '/PPO_' + env_name + \"_log_\" + str(run_num) + \".csv\"\n",
        "\n",
        "print(\"current logging run number for \" + env_name + \" : \", run_num)\n",
        "print(\"logging at : \" + log_f_name)\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "################### checkpointing ###################\n",
        "\n",
        "run_num_pretrained = 0      #### change this to prevent overwriting weights in same env_name folder\n",
        "\n",
        "directory = \"PPO_preTrained\"\n",
        "if not os.path.exists(directory):\n",
        "      os.makedirs(directory)\n",
        "\n",
        "directory = directory + '/' + env_name + '/'\n",
        "if not os.path.exists(directory):\n",
        "      os.makedirs(directory)\n",
        "\n",
        "\n",
        "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
        "print(\"save checkpoint path : \" + checkpoint_path)\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "############# print all hyperparameters #############\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(\"max training timesteps : \", max_training_timesteps)\n",
        "print(\"max timesteps per episode : \", max_ep_len)\n",
        "\n",
        "print(\"model saving frequency : \" + str(save_model_freq) + \" timesteps\")\n",
        "print(\"log frequency : \" + str(log_freq) + \" timesteps\")\n",
        "print(\"printing average reward over episodes in last : \" + str(print_freq) + \" timesteps\")\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(\"state space dimension : \", state_dim)\n",
        "print(\"action space dimension : \", action_dim)\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "if has_continuous_action_space:\n",
        "    print(\"Initializing a continuous action space policy\")\n",
        "    print(\"--------------------------------------------------------------------------------------------\")\n",
        "    print(\"starting std of action distribution : \", action_std)\n",
        "    print(\"decay rate of std of action distribution : \", action_std_decay_rate)\n",
        "    print(\"minimum std of action distribution : \", min_action_std)\n",
        "    print(\"decay frequency of std of action distribution : \" + str(action_std_decay_freq) + \" timesteps\")\n",
        "\n",
        "else:\n",
        "    print(\"Initializing a discrete action space policy\")\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(\"PPO update frequency : \" + str(update_timestep) + \" timesteps\")\n",
        "print(\"PPO K epochs : \", K_epochs)\n",
        "print(\"PPO epsilon clip : \", eps_clip)\n",
        "print(\"discount factor (gamma) : \", gamma)\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(\"optimizer learning rate actor : \", lr_actor)\n",
        "print(\"optimizer learning rate critic : \", lr_critic)\n",
        "\n",
        "if random_seed:\n",
        "    print(\"--------------------------------------------------------------------------------------------\")\n",
        "    print(\"setting random seed to \", random_seed)\n",
        "    torch.manual_seed(random_seed)\n",
        "    env.seed(random_seed)\n",
        "    np.random.seed(random_seed)\n",
        "\n",
        "#####################################################\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "################# training procedure ################\n",
        "\n",
        "# initialize a PPO agent\n",
        "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
        "\n",
        "\n",
        "# track total training time\n",
        "start_time = datetime.now().replace(microsecond=0)\n",
        "print(\"Started training at (GMT) : \", start_time)\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "# logging file\n",
        "log_f = open(log_f_name,\"w+\")\n",
        "log_f.write('episode,timestep,reward\\n')\n",
        "\n",
        "\n",
        "# printing and logging variables\n",
        "print_running_reward = 0\n",
        "print_running_episodes = 0\n",
        "\n",
        "log_running_reward = 0\n",
        "log_running_episodes = 0\n",
        "\n",
        "time_step = 0\n",
        "i_episode = 0\n",
        "\n",
        "\n",
        "# training loop\n",
        "while time_step <= max_training_timesteps:\n",
        "\n",
        "    state = env.reset()\n",
        "    #state = state[0] #\n",
        "    current_ep_reward = 0\n",
        "\n",
        "    for t in range(1, max_ep_len+1):\n",
        "\n",
        "        # select action with policy\n",
        "        action = ppo_agent.select_action(state)\n",
        "        state, reward, done = env.step(action)\n",
        "\n",
        "        # saving reward and is_terminals\n",
        "        ppo_agent.buffer.rewards.append(reward)\n",
        "        ppo_agent.buffer.is_terminals.append(done)\n",
        "\n",
        "        time_step +=1\n",
        "        current_ep_reward += reward\n",
        "\n",
        "        # update PPO agent\n",
        "        if time_step % update_timestep == 0:\n",
        "            ppo_agent.update()\n",
        "\n",
        "        # if continuous action space; then decay action std of ouput action distribution\n",
        "        if has_continuous_action_space and time_step % action_std_decay_freq == 0:\n",
        "            ppo_agent.decay_action_std(action_std_decay_rate, min_action_std)\n",
        "\n",
        "        # log in logging file\n",
        "        if time_step % log_freq == 0:\n",
        "\n",
        "            # log average reward till last episode\n",
        "            log_avg_reward = log_running_reward / log_running_episodes\n",
        "            log_avg_reward = round(log_avg_reward, 4)\n",
        "\n",
        "            log_f.write('{},{},{}\\n'.format(i_episode, time_step, log_avg_reward))\n",
        "            log_f.flush()\n",
        "\n",
        "            log_running_reward = 0\n",
        "            log_running_episodes = 0\n",
        "\n",
        "        # printing average reward\n",
        "        if time_step % print_freq == 0:\n",
        "\n",
        "            # print average reward till last episode\n",
        "            print_avg_reward = print_running_reward / print_running_episodes\n",
        "            print_avg_reward = round(print_avg_reward, 2)\n",
        "\n",
        "            print(\"Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward))\n",
        "\n",
        "            print_running_reward = 0\n",
        "            print_running_episodes = 0\n",
        "\n",
        "        # save model weights\n",
        "        if time_step % save_model_freq == 0:\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "            print(\"saving model at : \" + checkpoint_path)\n",
        "            ppo_agent.save(checkpoint_path)\n",
        "            print(\"model saved\")\n",
        "            print(\"Elapsed Time  : \", datetime.now().replace(microsecond=0) - start_time)\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "        # break; if the episode is over\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    print_running_reward += current_ep_reward\n",
        "    print_running_episodes += 1\n",
        "\n",
        "    log_running_reward += current_ep_reward\n",
        "    log_running_episodes += 1\n",
        "\n",
        "    i_episode += 1\n",
        "\n",
        "\n",
        "log_f.close()\n",
        "#env.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print total training time\n",
        "print(\"============================================================================================\")\n",
        "end_time = datetime.now().replace(microsecond=0)\n",
        "print(\"Started training at (GMT) : \", start_time)\n",
        "print(\"Finished training at (GMT) : \", end_time)\n",
        "print(\"Total training time  : \", end_time - start_time)\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FJwAgcVOpM1w"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZewQELovHFt4"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - IV**\n",
        "\n",
        "*   load log files using pandas\n",
        "*   plot graph using matplotlib\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "id": "bY-E5HGcGxiu",
        "outputId": "f359353c-4073-4331-f437-4f0d85b624d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "loading data from : PPO_logs/Inverted Pendulum in MuJoCo//PPO_Inverted Pendulum in MuJoCo_log_0.csv\n",
            "data shape :  (2500, 3)\n",
            "--------------------------------------------------------------------------------------------\n",
            "============================================================================================\n",
            "figure saved at :  PPO_figs/Inverted Pendulum in MuJoCo//PPO_Inverted Pendulum in MuJoCo_fig_0.png\n",
            "============================================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "\n",
        "fig_num = 0     #### change this to prevent overwriting figures in same env_name folder\n",
        "\n",
        "plot_avg = True    # plot average of all runs; else plot all runs separately\n",
        "\n",
        "fig_width = 10\n",
        "fig_height = 6\n",
        "\n",
        "\n",
        "# smooth out rewards to get a smooth and a less smooth (var) plot lines\n",
        "window_len_smooth = 50\n",
        "min_window_len_smooth = 1\n",
        "linewidth_smooth = 1.5\n",
        "alpha_smooth = 1\n",
        "\n",
        "window_len_var = 5\n",
        "min_window_len_var = 1\n",
        "linewidth_var = 2\n",
        "alpha_var = 0.1\n",
        "\n",
        "\n",
        "colors = ['red', 'blue', 'green', 'orange', 'purple', 'olive', 'brown', 'magenta', 'cyan', 'crimson','gray', 'black']\n",
        "\n",
        "\n",
        "# make directory for saving figures\n",
        "figures_dir = \"PPO_figs\"\n",
        "if not os.path.exists(figures_dir):\n",
        "    os.makedirs(figures_dir)\n",
        "\n",
        "# make environment directory for saving figures\n",
        "figures_dir = figures_dir + '/' + env_name + '/'\n",
        "if not os.path.exists(figures_dir):\n",
        "    os.makedirs(figures_dir)\n",
        "\n",
        "\n",
        "fig_save_path = figures_dir + '/PPO_' + env_name + '_fig_' + str(fig_num) + '.png'\n",
        "\n",
        "\n",
        "# get number of log files in directory\n",
        "log_dir = \"PPO_logs\" + '/' + env_name + '/'\n",
        "\n",
        "current_num_files = next(os.walk(log_dir))[2]\n",
        "num_runs = len(current_num_files)\n",
        "\n",
        "\n",
        "all_runs = []\n",
        "\n",
        "for run_num in range(num_runs):\n",
        "\n",
        "    log_f_name = log_dir + '/PPO_' + env_name + \"_log_\" + str(run_num) + \".csv\"\n",
        "    print(\"loading data from : \" + log_f_name)\n",
        "    data = pd.read_csv(log_f_name)\n",
        "    data = pd.DataFrame(data)\n",
        "\n",
        "    print(\"data shape : \", data.shape)\n",
        "\n",
        "    all_runs.append(data)\n",
        "    print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "ax = plt.gca()\n",
        "\n",
        "if plot_avg:\n",
        "    # average all runs\n",
        "    df_concat = pd.concat(all_runs)\n",
        "    df_concat_groupby = df_concat.groupby(df_concat.index)\n",
        "    data_avg = df_concat_groupby.mean()\n",
        "\n",
        "    # smooth out rewards to get a smooth and a less smooth (var) plot lines\n",
        "    data_avg['reward_smooth'] = data_avg['reward'].rolling(window=window_len_smooth, win_type='triang', min_periods=min_window_len_smooth).mean()\n",
        "    data_avg['reward_var'] = data_avg['reward'].rolling(window=window_len_var, win_type='triang', min_periods=min_window_len_var).mean()\n",
        "\n",
        "    data_avg.plot(kind='line', x='timestep' , y='reward_smooth',ax=ax,color=colors[0],  linewidth=linewidth_smooth, alpha=alpha_smooth)\n",
        "    data_avg.plot(kind='line', x='timestep' , y='reward_var',ax=ax,color=colors[0],  linewidth=linewidth_var, alpha=alpha_var)\n",
        "\n",
        "    # keep only reward_smooth in the legend and rename it\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    ax.legend([handles[0]], [\"reward_avg_\" + str(len(all_runs)) + \"_runs\"], loc=2)\n",
        "\n",
        "\n",
        "else:\n",
        "    for i, run in enumerate(all_runs):\n",
        "        # smooth out rewards to get a smooth and a less smooth (var) plot lines\n",
        "        run['reward_smooth_' + str(i)] = run['reward'].rolling(window=window_len_smooth, win_type='triang', min_periods=min_window_len_smooth).mean()\n",
        "        run['reward_var_' + str(i)] = run['reward'].rolling(window=window_len_var, win_type='triang', min_periods=min_window_len_var).mean()\n",
        "\n",
        "        # plot the lines\n",
        "        run.plot(kind='line', x='timestep' , y='reward_smooth_' + str(i),ax=ax,color=colors[i % len(colors)],  linewidth=linewidth_smooth, alpha=alpha_smooth)\n",
        "        run.plot(kind='line', x='timestep' , y='reward_var_' + str(i),ax=ax,color=colors[i % len(colors)],  linewidth=linewidth_var, alpha=alpha_var)\n",
        "\n",
        "    # keep alternate elements (reward_smooth_i) in the legend\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    new_handles = []\n",
        "    new_labels = []\n",
        "    for i in range(len(handles)):\n",
        "        if(i%2 == 0):\n",
        "            new_handles.append(handles[i])\n",
        "            new_labels.append(labels[i])\n",
        "    ax.legend(new_handles, new_labels, loc=2)\n",
        "\n",
        "\n",
        "\n",
        "# ax.set_yticks(np.arange(0, 1800, 200))\n",
        "# ax.set_xticks(np.arange(0, int(4e6), int(5e5)))\n",
        "\n",
        "\n",
        "ax.grid(color='gray', linestyle='-', linewidth=1, alpha=0.2)\n",
        "\n",
        "ax.set_xlabel(\"Timesteps\", fontsize=12)\n",
        "ax.set_ylabel(\"Rewards\", fontsize=12)\n",
        "\n",
        "plt.title(env_name, fontsize=14)\n",
        "\n",
        "\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(fig_width, fig_height)\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "plt.savefig(fig_save_path)\n",
        "print(\"figure saved at : \", fig_save_path)\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "history_visible": true
    },
    "kernelspec": {
      "display_name": "RL",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}